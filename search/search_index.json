{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ECNN - Embedded Convolutional Neural Network","text":"<p>Essa \u00e9 a parte te\u00f3rica do Tutorial de constru\u00e7\u00e3o de um modelo de coloriza\u00e7\u00e3o de imagens classificadas. Esse modelo dever\u00e1 receber uma imagem em preto e branco (escala de cinza) e, utilizando o <code>tema da imagem</code>, tentar\u00e1 colorir essa imagem.</p> <p>A parte pr\u00e1tica desse tutorial est\u00e1 dispon\u00edvel neste notebook.</p> <p>Os c\u00f3digos presentes no tutorial foram feita utilizando a biblioteca PyTorch. O treinamento poder\u00e1 ser feito utilizando o conjunto de treinamento do dataset Image Colorization Dataset e a classifica\u00e7\u00e3o manual dessas imagens feita por n\u00f3s, disponivel em: Categories.</p> <p>As seguintes categorias foram usadas na classifica\u00e7\u00e3o das imagens: <code>pessoas</code>, <code>alimentos</code>, <code>animais</code>, <code>veiculos</code>, <code>ambientes externos</code>, <code>ambientes internos</code> e <code>objetos</code>.</p> <p>Alerta</p> <p>\u00c9 extremamente recomendado que a constru\u00e7\u00e3o final do c\u00f3digo presente no Notebbok s\u00f3 seja feita ap\u00f3s a leitura completa desta parte te\u00f3rica.</p>"},{"location":"#creditos","title":"Cr\u00e9ditos","text":""},{"location":"#desenvolvedores","title":"Desenvolvedores","text":"<ul> <li>Beatriz Rodrigues de Freitas</li> <li>Carlos Eduardo Porciuncula Yamada</li> <li>Rafael Dourado Bastos de Oliveira</li> </ul>"},{"location":"#orientacao","title":"Orienta\u00e7\u00e3o","text":"<ul> <li>F\u00e1bio Jos\u00e9 Ayres</li> </ul>"},{"location":"batches_epochs/","title":"Batches e Epochs","text":""},{"location":"batches_epochs/#batches","title":"Batches","text":"<p>Os chamados batches s\u00e3o amostras agrupadas que s\u00e3o usadas no modelo a cada vez que os par\u00e2metros dele mudam, ou seja, para cada conjunto de par\u00e2metros, teremos um batch e assim por diante.</p> <p>Ao final de cada batch, \u00e9 calculado o erro utilizando uma loss function (como o MSE) e a partir desse erro, o algoritmo atualiza o modelo a fim de melhor\u00e1-lo.</p> <p>O <code>batch size</code> \u00e9 o hiperpar\u00e2metro que define o n\u00famero de amostras por batch que ser\u00e3o utilizadas durante as itera\u00e7\u00f5es.</p> <p>Existem tr\u00eas tipos de algoritmos de aprendizado com base no <code>batch size</code>, sendo eles:</p> <ol> <li> <p>Gradiente Descendente por Batch: quando o <code>batch size</code> \u00e9 do tamanho da base de treinamento;</p> </li> <li> <p>Gradiente Descendente Estoc\u00e1stico: quando o <code>batch size</code> \u00e9 igual a 1;</p> </li> <li> <p>Gradiente Descendente por Mini-Batch: quando o <code>batch size</code> \u00e9 um valor entre 1 e o tamanho da base de treinamento (valor diferente de cada um dos extremos).</p> </li> </ol> <p>Em nosso modelo, por exemplo, utilizamos um <code>batch size</code> de 32 imagens, ou seja, utilizamos o algoritmo de gradiente descendente por mini-batch para aprendizagem do modelo.</p> <p>Os Batches s\u00e3o criados a partir da separa\u00e7\u00e3o de uma base de dados em conjuntos menores de dados por meio de um <code>DataLoader</code>.Normalmente, o <code>DataLoader</code> gera os Batches de forma aleatorizada e os carrega na mem\u00f3ria (seja ela a RAM, cache, ou da pr\u00f3pria GPU) para que o modelo possa utiliza-los. Esse processo de carregamento \u00e9 potencialmente um dos mais lentos durante o treinamento, por\u00e9m ele pode ser acelerado por meio da utliza\u00e7\u00e3o do multiprocessamento que discutiremos mais para frente.</p> <p>C\u00f3digo: DataLoader</p> <p>Um <code>DataLoader</code> pode ser declarado da seguinte forma: <pre><code>from torch.utils.data import DataLoader, Dataset\n\n# Classe de um dataset personalizado capaz de carregar as imagens da base de dados\nclass ImageDataset(Dataset):\n    ...\n\ndataloader = DataLoader(\n    ImageDataset(), # Dataset de imagens \n    batch_size=32, # tamanho dos batchs\n    shuffle=True, # batchs formados aleatoriamente\n    # ... par\u00e2metros de multiprocessamento\n)\n</code></pre></p>"},{"location":"batches_epochs/#epochs","title":"Epochs","text":"<p>Uma epoch (ou \u00e9poca) \u00e9 o hiperpar\u00e2metro que define a quantidade de vezes na qual o algoritmo de aprendizagem anteriormente explicado ser\u00e1 computado ao longo da amostra de treinamento. Para nos situarmos melhor, podemos imaginar que seria feito um <code>for-loop</code> para executar o treinamento por uma determinada quantidade de epochs.</p> <p>C\u00f3digo: For-loop com epochs</p> <p>O c\u00f3digo abaixo esbo\u00e7a um c\u00f3digo de treinamento de um modelo utilizando multiplas Epochs e Batches:</p> <pre><code># ... c\u00f3digo acima\n# imports, Declara\u00e7\u00e3o do modelo, dataset, otimizador...\n\nfor epoch in range(epochs):\n    for i, batch in enumerate(dataloader): \n\n        # Rotina de Treinamento\n\n# Finaliza\u00e7\u00e3o do Treino e come\u00e7o do Teste\n# ... c\u00f3digo abaixo\n</code></pre>"},{"location":"batnorm_actfunc/","title":"Normaliza\u00e7\u00e3o em Batches e Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o","text":""},{"location":"batnorm_actfunc/#normalizacao-em-batches-batch-normalization","title":"Normaliza\u00e7\u00e3o em Batches (Batch normalization)","text":"<p>A normaliza\u00e7\u00e3o em Batches (Lotes) surgiu com o prop\u00f3sito de remediar o problema de mudan\u00e7a de covari\u00e1vel interna (em ingl\u00eas, internal covariant shift problem). Esse problema \u00e9 causado pela varia\u00e7\u00e3o da distribui\u00e7\u00e3o dos par\u00e2metros das entradas, que pode dificultar na converg\u00eancia do modelo, uma vez que torna o processo de treinamento mais lento do que deveria ser.</p> <p>Quando \u00e9 feita a normaliza\u00e7\u00e3o em mini-batches (mini-lotes), isto \u00e9, quando a m\u00e9dia \u00e9 ajustada para 0 e a vari\u00e2ncia para 1, s\u00e3o utilizados dois novos par\u00e2metros, o deslocamento e a escala, que servem para otimizar a normaliza\u00e7\u00e3o para as ativa\u00e7\u00f5es. Dessa forma, o processo de aprendizado da rede neural \u00e9 estabilizado, reduzindo a mudan\u00e7a de covari\u00e1vel interna, garantindo consist\u00eancia entre as camadas do modelo.</p> <p>Para criar uma camada de normaliza\u00e7\u00e3o em Batches \u00e9 obrigat\u00f3rio apenas especificar a quantidade de caracter\u00edsticas (features) que ser\u00e3o normalizadas. No caso das imagens, essas caracter\u00edsticas podem ser os layers da imagem.</p> <pre><code>from torch import nn\n\nbn = nn.BatchNorm2d(32)\n</code></pre>"},{"location":"batnorm_actfunc/#funcoes-de-ativacao-activation-functions","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o (Activation Functions)","text":"<p>Em cada camada, seja ela de convolu\u00e7\u00e3o ou convolu\u00e7\u00e3o transposta, teremos v\u00e1rios n\u00f3s. Como \u00e9 caracter\u00edstico da rede neural, um n\u00f3 receber\u00e1 como entrada os valores de sa\u00edda de n\u00f3s da camada anterior. O valor de sa\u00edda do n\u00f3 atual depender\u00e1 de uma soma de um valor \"bias\" com os valores ponderados da camada anterior. </p> <p>Entretanto, esse valor de sa\u00edda ser\u00e1 ajustado por uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o, que determina se o valor calculado ir\u00e1 ou n\u00e3o ser propagado para a pr\u00f3xima camada. No nosso modelo, a fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 a <code>ReLU</code> (Rectified Linear Unit), que determina que a sa\u00edda do n\u00f3 ser\u00e1 a soma calculada caso ela seja maior do que 0, e 0 caso contr\u00e1rio. Essa fun\u00e7\u00e3o de ativa\u00e7\u00e3o reduz o problema do desaparecimento de gradientes, que causaria perda de informa\u00e7\u00e3o na rede.</p> <p>Uma vez que o aprendizado do modelo \u00e9 justamente determinar os pesos ideais para essa soma, o valor de sa\u00edda do n\u00f3 dever\u00e1 mudar a cada intera\u00e7\u00e3o.</p>"},{"location":"batnorm_actfunc/#referencias","title":"Refer\u00eancias","text":"<ol> <li>What is Batch Normalization In Deep Learning?</li> <li>Internal Covariant Shift Problem in Deep Learning</li> </ol> <p>https://www.geeksforgeeks.org/activation-functions-neural-networks/</p> <p>https://medium.com/@meetkp/understanding-the-rectified-linear-unit-relu-a-key-activation-function-in-neural-networks-28108fba8f07</p>"},{"location":"context/","title":"Contexto","text":"<p>Iremos desenvolver um modelo generativo para coloriza\u00e7\u00e3o de imagens. Como j\u00e1 \u00e9 esperado, isso significa que o modelo finalizado ter\u00e1 como entrada uma imagem preto e branco e retornar\u00e1 uma imagem colorida. Entretanto, teremos mais uma entrada al\u00e9m da imagem em preto e branco, uma categoria.</p> <p>Alguns padr\u00f5es s\u00e3o facilmente reconhecidos como por exemplo, ambientes externos normalmente ter\u00e3o mais verde devido a presen\u00e7a de plantas ou azul devido ao c\u00e9u, enquanto ambinete internos normalmente ter\u00e3o cores mais neutras. Adicionando a categoria como entrada, permitiremos que o modelo indentifique esses padr\u00f5es mais espec\u00edficos e os levem em conta na hora de colorir uma nova imagem.</p> <p>Com base no dataset Image Colorization Dataset definimos as categorias como:</p> <ul> <li>Ambiente externo</li> <li>Ambiente interno</li> <li>Objeto</li> <li>Pessoa</li> <li>Animal</li> <li>Ve\u00edculo</li> <li>Comida </li> </ul>"},{"location":"convolutions/","title":"Tipos de Convolu\u00e7\u00f5es","text":""},{"location":"convolutions/#camada-de-convolucao","title":"Camada de Convolu\u00e7\u00e3o","text":"<p>Convolu\u00e7\u00f5es s\u00e3o processos em que um kernel (uma matriz de tamanho \\(n \\times m\\)) percorre os campos receptivos de uma imagem, gerando um output com altura e largura menores que a imagem original.</p> <p>Durante uma convolu\u00e7\u00e3o, existem alguns par\u00e2metros que podemos alterar para que alcancemos o resultado que desejamos e que usamos em nosso modelo, sendo eles:</p> <ol> <li> <p>Padding: par\u00e2metro que cria uma esp\u00e9cie de \"moldura\" de zeros que \u00e9 colocada em volta da entrada. Existem 3 tipos de padding: v\u00e1lido, igual ou completo. Utilizamos em nosso modelo o padding completo, de tamanho 1.</p> </li> <li> <p>Stride: \u00e9 o passo no qual o kernel percorrer\u00e1 a imagem. Quanto maior o valor do passo, menor ser\u00e1 o output. Por exemplo, no nosso modelo, utilizamos um passo de 2, para que o output seja menor que a imagem original. Nesse tutorial, quando quisermos alterar o tamanho da imagem usaremos <code>stride=2</code>, assim diminuindo as dimens\u00f5es da imagem pela metade ou dobrando-as.</p> </li> <li> <p>Dilation: processo que faz com que o campo receptivo seja aumentado inserindo espa\u00e7amentos entre os pixels do kernel. Assim, da mesma maneira que um kernel de 5x5 com dilata\u00e7\u00e3o 1 possui 25 pesos, um kernel de 3x3 com dilata\u00e7\u00e3o 2 possui 9 pesos, ou seja, diminu\u00edmos a quantidade de par\u00e2metros e garantimos que seja capturada a mesma quantidade de informa\u00e7\u00e3o.</p> </li> </ol> <p>As camadas de convolu\u00e7\u00e3o s\u00e3o utilizadas para transformar uma entrada em valores num\u00e9ricos, que ser\u00e3o interpretados durante o processo de treinamento da rede neural.</p> <p>C\u00f3digo: Convolu\u00e7\u00f5es</p> <p>Esse conceito pode ser aplicado em Python, como demonstrado abaixo:</p> Sem Dilata\u00e7\u00e3oCom Dilata\u00e7\u00e3o <p>No c\u00f3digo, para declarar uma camada de convolu\u00e7\u00e3o \u00e9 necess\u00e1rio especificar, nessa ordem, a quantidade de layers de entrada, quantidade de layers de sa\u00edda, tamanho do kernel, tamanho do passo (stride) (padr\u00e3o=1) e tamanho da moldura (padding) (padr\u00e3o=0). Nos modelos construidos nesse tutorial utilizaremos <code>kernel_size=4</code> e <code>padding=1</code>.</p> <pre><code>from torch import nn\n\nconv = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n</code></pre> <p>J\u00e1 para declarar uma camada de dilata\u00e7\u00e3o \u00e9 necess\u00e1rio especificar, al\u00e9m dos par\u00e2metros da convolu\u00e7\u00e3o normal: a quantidade de pesos de dilata\u00e7\u00e3o (padr\u00e3o=1). Nos modelos constru\u00eddos nesse tutorial utilizaremos <code>dilation=2</code> com <code>padding=3</code>. <pre><code>from torch import nn\n\ndilat = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n</code></pre></p>"},{"location":"convolutions/#camada-de-convolucao-transposta","title":"Camada de Convolu\u00e7\u00e3o Transposta","text":"<p>Uma vez que reduzimos as dimens\u00f5es espaciais da nossa amostra atrav\u00e9s de convolu\u00e7\u00f5es, precisamos de uma ferramenta para voltar ao estado inicial da imagem, que \u00e9 o objetivo de nossa modelagem.</p> <p>A convolu\u00e7\u00e3o transposta funciona de forma que, quando temos uma entrada de dimens\u00f5es \\(n_h \\times n_w\\) e um kernel de dimens\u00f5es \\(k_h \\times k_w\\), utilizando um passo de 1 (<code>stride = 1</code>), o kernel percorrer\u00e1 um total de \\(n_h n_w\\) vezes, produzindo esse mesmo valor de resultados intermedi\u00e1rios, sendo que cada um possui dimens\u00f5es de \\((n_h + k_h - 1) \\times (n_w + k_w - 1)\\).</p> <p></p> <p>Dessa forma, conseguimos recuperar a imagem aumentando sua resolu\u00e7\u00e3o a cada itera\u00e7\u00e3o. Considerando que existem \\(n\\) camadas convolucionais, ser\u00e3o feitas \\(n\\) camadas convolucionais transpostas para recuperar a imagem anteriormente reduzida para treinamento, sendo que na \u00faltima itera\u00e7\u00e3o, obtemos os dois canais de cor desejados, A e B, que junto \u00e0 camada L inicial, comp\u00f5em a imagem.</p> <p>Sua sintaxe no c\u00f3digo \u00e9 muito parecida coma a da camada de convolu\u00e7\u00e3o, possuindo os mesmos par\u00e2metros: C\u00f3digo: Convolu\u00e7\u00e3o transposta<pre><code>from torch import nn\n\ntconv = nn.ConvTranspose2d(32, 2, kernel_size=4, stride=2, padding=1)\n</code></pre></p>"},{"location":"convolutions/#referencias","title":"Refer\u00eancias","text":"<ol> <li>What are convolutional neural networks?</li> <li>Dilated Convolution</li> <li>Transposed Convolution - Basic Operation</li> </ol>"},{"location":"embeddings/","title":"Camada de Embeddings","text":"<p><code>Embeddings</code> s\u00e3o representa\u00e7\u00f5es lineares e condensadas de dados discretos (como palavras ou categorias), esses dados s\u00e3o traduzidos em uma f\u00f3rmula matem\u00e1tica, de forma a enfatizar ou n\u00e3o caracter\u00edsticas que eles podem possuir, tornando poss\u00edvel, assim, agrupar dados com caracteriscas similares. </p> <p>Utilizaremos <code>Embeddings</code> para treinar nosso modelo a reconhecer caracteristicas comuns em certos tipos de imagens, utilizando as categorias que citamos anteriormente. Isso nos ser\u00e1 util, pois, uma vez que o modelo aprenda que que imagens categorizadas com sendo de ambientes externos possuem a caracteristica de ter um c\u00e9u, e que esse c\u00e9u costuma ser azul, ele colorir\u00e1 o c\u00e9u mais facilmente.</p> <p>Para criar uma camada de <code>Embeddings</code> no c\u00f3digo, precisamos obrigat\u00f3riamente especificar quantos tipos de dados diferentes possuimos, por exemplo: quantas categorias possuimos, e quantas caracteristicas cada <code>Embedding</code> ter\u00e1.</p> Criando uma camada de Embeddings<pre><code>from torch import nn\n\n# criando uma camada de Embeddings com 8 embeddins e 10 carcateristicas\nembd = nn.Embedding(8, 10)\n</code></pre>"},{"location":"embeddings/#referencias","title":"Refer\u00eancias","text":"<ol> <li>O que s\u00e3o embeddings em Machine Learning</li> </ol>"},{"location":"inputs_outputs/","title":"Entradas e Sa\u00eddas do Modelo","text":"<p>Queremos treinar um modelo capaz de transformar imagens em preto e branco (escala de cinza) em imagens coloridas. Dessa forma, mesmo ele sendo generativo \u00e9 mais correto afirmar que nosso modelo realizar\u00e1 uma melhoria na imagem, ou inv\u00e9s de criar uma totalmente nova.</p>"},{"location":"inputs_outputs/#entradas-do-modelo","title":"Entradas do Modelo","text":"<p>A imagens s\u00e3o conjuntos de dados constitu\u00eddos de 3 dimens\u00f5es: Altura, Largura e uma terceira dimens\u00e3o que consiste nas camadas que formam a cor da imagem. Formatos comuns, como <code>PNG</code> ou <code>JPG</code>, possuem essa dimens\u00e3o de cor no sistema <code>RGB</code>, que divide essa dimens\u00e3o em tr\u00eas camadas: Red Green Blue. Mesmo imagens em preto e branco, quando nesses formatos, possuem essas tr\u00eas camadas, por\u00e9m elas poderiam ser representadas por apenas uma.</p> <p>Uma outra forma de representar camadas de cor \u00e9 o sistema <code>LAB</code> que consiste em uma camada de Luminosidade e duas camadas de cor: camada A, que representa o equil\u00edbrio entre o Vermelho e o Verde, e a camada B que representa o equil\u00edbrio entre o Amarelo e o Azul. Utilizando esse sistema, \u00e9 poss\u00edvel transformar imagens coloridas em imagens em preto e branco com uma \u00fanica camada, para isso \u00e9 necess\u00e1rio converter do sistema <code>RGB</code> para o <code>LAB</code> e remover suas duas camadas de cor, a A e a B.</p> Trantando a Imagem de Entrada<pre><code>img_path = \"./exemplo.jpg\"\n\n# l\u00ea uma imagem criando um vetor de 3 dimens\u00f5es (Largura, Altura, Cor)\nimg = imread(img_path) \n\n# converte do sistema RGB para o LAB e a transforma em um Tensor\nLAB_img = from_numpy(rgb2lab(img)) \n\n# reorganiza as dimens\u00f5es da imagem para o formato do pytorch \nLAB_img = LAB_img.permute(2, 0, 1) # (Cor, Largura, Altura)\n\n# separa apenas a camada L\ngray_layer = LAB_img[0, :, :].unsqueeze(0) \n\n# separa as camadas A e B\ncolor_layers = LAB_img[1:, :, :] \n</code></pre> <p>Como nosso objetivo final \u00e9 construir um modelo capaz de colorir imagens em preto e branco previamente categoriazadas, a outra entrada do modelo dever\u00e1 ser a categoria na qual a imagem se encaixa. Essa categoria dever\u00e1 ser um n\u00famero inteiro que represente unicamente aquela categoria.</p> <p>Nas categorias que usaremos, temos:</p> <ol> <li>Comida </li> <li>Animal</li> <li>Pessoa</li> <li>Objeto</li> <li>Ve\u00edculo</li> <li>Ambiente interno</li> <li>Ambiente externo</li> </ol>"},{"location":"inputs_outputs/#saida-do-modelo","title":"Sa\u00edda do Modelo","text":"<p>Ap\u00f3s o processamento do modelo, ele nos devolver\u00e1 duas novas camadas criadas a partir da camada L. Utilizaremos essas camadas como se fossem as camadas A e B faltantes junto a camada L original para formar a imagem colorida.</p> <p>Uma imagem <code>LAB</code> n\u00e3o \u00e9 muito utilizada, por isso devemos transformar a imagem de volta em <code>RGB</code> antes de utiliza-la. Para isso basta seguir o processo inverso de se obter a imagem LAB:</p> Trantando a Sa\u00edda do Modelo<pre><code>### C\u00f3digo omitido acima\n\npred_color_layers = model(gray_layer)\n\n# concatena a camada L com as camadas AB\npred_LAB_img = torch.cat((gray_layer, pred_color_layers))\n\n# reorganiza as dimens\u00f5es para imagem para o formato padr\u00e3o \npred_LAB_img = pred_LAB_img.permute(1, 2, 0) # (Largura, Altura, Cor) \n\n# converte do sistema LAB para o RGB \npred_RGB_img = lab2rgb(pred_LAB_img)\n</code></pre> C\u00f3digo completo C\u00f3digo completo<pre><code>from torch import from_numpy, cat\nfrom skimage.io import imread\nfrom skimage.color import rgb2lab, lab2rgb\n\n# l\u00ea a imagem\nimg_path = \"./exemplo.jpg\"\nimg = imread(img_path) \n\n# converte a imagem de RGB para LAB\nLAB_img = from_numpy(rgb2lab(img)) \nLAB_img = LAB_img.permute(2, 0, 1) \n\n# separa os layers\ngray_layer = LAB_img[0, :, :].unsqueeze(0) \ncolor_layers = LAB_img[1:, :, :] \n\n# aplica o modelo de coloriza\u00e7\u00e3o\npred_color_layers = model(gray_layer)\n\n# reune os layers\npred_LAB_img = torch.cat((gray_layer, pred_color_layers))\n\n# converte a imagem de LAB para RGB\npred_LAB_img = pred_LAB_img.permute(1, 2, 0) \npred_RGB_img = lab2rgb(pred_LAB_img)\n</code></pre>"},{"location":"inputs_outputs/#referencias","title":"Refer\u00eancias","text":"<ol> <li>Generative Models and Autoencoders</li> </ol>"},{"location":"model_type/","title":"Tipo de Modelo","text":"<p>O modelo desenvolvido \u00e9 do tipo <code>Autoencoder</code>, ou seja, ele ter\u00e1 duas etapas chamadas Encoder e Decoder. No Encoder, a informa\u00e7\u00e3o de entrada \u00e9 reduzida para uma dimens\u00e3o menor enquanto no Decoder essa informa\u00e7\u00e3o reduzida ser\u00e1 utilizada para reconstruir a informa\u00e7\u00e3o de sa\u00edda com o tamanho original. H\u00e1 diferentes formas de realizar o desenvolvimento necess\u00e1rio nessas duas etapas, nesse caso cada etapa ser\u00e1 desenvolvida usando uma <code>Rede Neural Convolucional</code>.</p> <p>A <code>Rede Neural Convolucional</code> \u00e9 um tipo de Rede Neural, dessa forma, a informa\u00e7\u00e3o ir\u00e1 fluir da entrada para a sa\u00edda passando por camadas de n\u00f3s, em que cada n\u00f3 transforma a informa\u00e7\u00e3o recebida e passa adiante. </p> <p>A diferen\u00e7a entre a <code>rede neural convolucional</code> e a regular est\u00e1 em como essa informa\u00e7\u00e3o \u00e9 passada. Na rede neural regular, todos os n\u00f3s de uma camada est\u00e3o ligados com todos os n\u00f3s da camada seguinte, j\u00e1 na convolucional, as informa\u00e7\u00f5es de um n\u00f3 s\u00e3o passados para apenas alguns dos n\u00f3s da camada seguinte, permitindo que cada parte da informa\u00e7\u00e3o seja analisada separadamente. </p> <p>No caso da imagem, a separa\u00e7\u00e3o da informa\u00e7\u00e3o \u00e9 feita pelo field que ser\u00e1 explicado mais adiante. Essa separa\u00e7\u00e3o \u00e9 muito utilizada na an\u00e1lise de imagens pois, com isso, apenas a informa\u00e7\u00e3o necess\u00e1ria para identificar um determinado padr\u00e3o \u00e9 passada adiante, reduzindo o n\u00famero de par\u00e2metros do modelo e tornando-o mais eficiente.</p>"},{"location":"model_type/#referencias","title":"Refer\u00eancias","text":"<ol> <li> <p>Convolutional Neural Networks for Computer Vision</p> </li> <li> <p>Generative Models and Autoencoders</p> </li> </ol>"},{"location":"multiprocessing/","title":"Multiprocessamento","text":"<p>O multiprocessamento \u00e9 utilizado para que o nosso programa seja executado de maneira mais eficiente, visto que a quantidade de dados de entrada \u00e9 grande e um fator importante para tornar a execu\u00e7\u00e3o do algoritmo vi\u00e1vel \u00e9 a velocidade com a qual ele ser\u00e1 executado.</p> <p>Dessa forma, a biblioteca do PyTorch disponibiliza ferramentas de paraleliza\u00e7\u00e3o quando carregamos nossos dados. O objeto <code>DataLoader</code> possui atributos que permitem o uso de multiprocessamento, os principais s\u00e3o:</p> <ol> <li><code>num_workers</code>: define a quantidade de subprocessos iremos utilizar para carregar os dados;</li> <li><code>pin_memory</code>: define se os dados ser\u00e3o copiados para o espa\u00e7o da mem\u00f3ria fixado do dispositivo (CPU ou GPU) para transfer\u00eancia dos dados do modelo; </li> <li><code>prefetch_factor</code>: define a quantidade de batches que ser\u00e3o carregados antecipadamente por cada subprocesso.</li> </ol> <p>Os par\u00e2metros citados acima s\u00e3o recomendados principalmente quando estamos treinando o modelo utilizando a GPU e com uma quantidade razo\u00e1vel de batches por epoch. Nessas condi\u00e7\u00f5es, a combina\u00e7\u00e3o certa desses par\u00e2metros tornar\u00e1 a execu\u00e7\u00e3o do c\u00f3digo mais eficiente, economizando tempo de processamento.</p> <p>Os subprocessos, ap\u00f3s v\u00e1rias itera\u00e7\u00f5es, consumir\u00e3o a mesma quantidade de mem\u00f3ria da CPU que o processo pai. Isso justifica poss\u00edveis momentos em que a execu\u00e7\u00e3o parece \"engasgar\" quando tratamos de um conjunto de treinamento grande e/ou quando s\u00e3o utilizados grandes n\u00fameros de subprocessos. Existem solu\u00e7\u00f5es para esse tipo de problema que s\u00e3o abordados da documenta\u00e7\u00e3o da biblioteca (vide refer\u00eancia).</p>"},{"location":"multiprocessing/#referencias","title":"Refer\u00eancias","text":"<ol> <li>PyTorch documentation &gt; torch.utils.data</li> </ol>"},{"location":"optimization/","title":"Otimizando os Par\u00e2metros do Modelo","text":"<p>Durante o treinamento do modelo, \u00e9 necess\u00e1rio ajustar os par\u00e2metros do modelo para minimizar o erro. Para isso, utilizamos t\u00e9cnicas de otimiza\u00e7\u00e3o que ajustam os par\u00e2metros do modelo de acordo com o erro calculado por uma fu\u00e7\u00e3o de perda.</p>"},{"location":"optimization/#funcao-de-perda","title":"Fun\u00e7\u00e3o de Perda","text":"<p>As fun\u00e7\u00f5es de perda, ou loss functions, s\u00e3o m\u00e9tricas utilizadas em Machine Learning para medir a performance de um modelo utilizando os valores que foram previstos por ele em compara\u00e7\u00e3o aos valores reais da base de dados.</p> <p>Uma fun\u00e7\u00e3o muito comumente usada em diversos modelos de previs\u00e3o (e ser\u00e1 a que utilizaremos nesse tutorial) \u00e9 o Mean Squared Error (<code>MSE</code>), que \u00e9 calculado pela diferen\u00e7a da valor obtido pela previs\u00e3o e o valor real presente na base de dados. A equa\u00e7\u00e3o dessa m\u00e9trica \u00e9 dada por:</p> \\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)^2\\] <p>Sendo \\(y_i\\) o valor real e \\(\\hat y_i\\) o valor previsto.</p> <p>Exemplo: Fun\u00e7\u00e3o de Perda</p> <p>Um exemplo simples da utiliza\u00e7\u00e3o da fun\u00e7\u00e3o de perda pode ser visto abaixo:</p> <pre><code>from torch import nn\n\ncriterion = nn.MSELoss() # Declara\u00e7\u00e3o da fun\u00e7\u00e3o de perda\n\ny = torch.randn(3, 5) # tensor aleatorio\ny_pred = torch.randn(3, 5, requires_grad=True) # tensor aleatorio\n\nloss = criterion(y_pred, y) # calculo da perda com MSE\nloss.backward() # Obten\u00e7\u00e3o da derivada\n</code></pre>"},{"location":"optimization/#gradiente-decendente","title":"Gradiente Decendente","text":"<p>Ap\u00f3s calcular o erro do modelo pela fun\u00e7\u00e3o de perda, \u00e9 necess\u00e1rio ajustar os par\u00e2metros do modelo para minimizar esse erro. A t\u00e9cnica de gradiente descendente ir\u00e1 determinar como devem ser feitos esses ajustes utilizando o vetor gradiente da fun\u00e7\u00e3o de perda. </p> <p>O vetor gradiente \u00e9 calculado atrav\u00e9s das derivadas parciais da fun\u00e7\u00e3o e o seu valor em cada ponto indicar\u00e1 a dire\u00e7\u00e3o em que o crescimento da fun\u00e7\u00e3o \u00e9 m\u00e1ximo (ou seja, o inverso do vetor gradiente indica a dire\u00e7\u00e3o em que est\u00e1 o m\u00ednimo da fun\u00e7\u00e3o).</p> <p>Com a informa\u00e7\u00e3o obtida atrav\u00e9s do vetor gradiente, pode-se determinar quais vari\u00e1veis do modelo devem ser alteradas e se elas devem aumentar ou diminuir para resultar na minimiza\u00e7\u00e3o do erro.</p> <p>O tamanho do passo que ser\u00e1 dado para ajustar as vari\u00e1veis do modelo \u00e9 determinado pelo <code>learning rate</code>. Quanto maior o <code>learning rate</code>, maior ser\u00e1 o passo e mais r\u00e1pido o modelo ir\u00e1 convergir para o erro m\u00ednimo, por\u00e9m, se o <code>learning rate</code> for muito grande, o modelo pode n\u00e3o convergir para o m\u00ednimo e oscilar em torno dele. Por outro lado, se o <code>learning rate</code> for muito pequeno, o modelo pode demorar muito para convergir para o m\u00ednimo.</p> <p>Para entender melhor tudo isso, vamos supor que o resultado do modelo seja dado por uma fun\u00e7\u00e3o \\(f\\) de tr\u00eas vari\u00e1veis \\(a, b, c\\) e que o modelo tem um <code>learning rate</code> de \\(\u03b1\\).</p> \\[f(a,b,c)\\] <p>O vetor gradiente dessa fun\u00e7\u00e3o ser\u00e1 dado por:</p> \\[(\\frac{\u2202f}{\u2202a}, \\frac{\u2202f}{\u2202b}, \\frac{\u2202f}{\u2202c})\\] <p>Com o gradiente decendente a altera\u00e7\u00e3o das vari\u00e1veis para chegar ao erro m\u00ednimo seria feita da seguinte forma:</p> \\[a_{otim} = a - \u03b1\\frac{\u2202f}{\u2202a}\\] \\[b_{otim} = b - \u03b1\\frac{\u2202f}{\u2202b}\\] \\[c_{otim} = c - \u03b1\\frac{\u2202f}{\u2202c}\\] <p>N\u00e3o parece complicado, mas no caso de redes neurais em que as vari\u00e1veis de uma camada dependem das vari\u00e1veis das camadas anteriores, o ajuste feito pelo Gradiente Decendente se torna mais complexo, pois requer que o c\u00e1lculo das derivadas parciais da fun\u00e7\u00e3o seja propagado em todas as camadas da rede.Essa propaga\u00e7\u00e3o \u00e9 chamada de back-propagation e no nosso modelo ser\u00e1 feita pela fun\u00e7\u00e3o backward do torch.</p>"},{"location":"optimization/#back-propagation","title":"Back-Propagation","text":"<p>Como explicado no t\u00f3pico acima, em uma fun\u00e7\u00e3o de perda no formato \\(f(a,b,c)\\), calcular a derivada da fun\u00e7\u00e3o de perda \u00e9 suficiente para atualizar os par\u00e2metros \\(a\\), \\(b\\) e \\(c\\) de forma a minimizar o erro do modelo.</p> <p>Entretanto, no caso da rede neural, o formato da fun\u00e7\u00e3o de perda \u00e9 mais parecido com:</p> \\[g(h(a,b), j(a,b), k(a,b))\\] <p>Em que a fun\u00e7\u00e3o final depende de outras fun\u00e7\u00f5es. Isso porque, cada camada, ao inv\u00e9s de receber diretamente par\u00e2metros do modelo, recebem as sa\u00eddas dos n\u00f3s da camada anterior. Apenas a primeira camada recebe apenas par\u00e2metros do modelo.</p> <p>Dessa forma, para determinar o impacto de cada par\u00e2metro no erro do modelo, apenas a derivada da fun\u00e7\u00e3o de perda n\u00e3o \u00e9 suficiente, essa derivada dever\u00e1 ser propagada por todas as camadas, isso \u00e9 chamado back-propagation.</p> <p>Vamos usar como exemplo a fun\u00e7\u00e3o, que seria o formato da fun\u00e7\u00e3o de perda de uma rede neural convolucional bem simples, com duas camadas.</p> \\[g(h_1(a,b), h_2(c,d))\\] <p>Em que as fun\u00e7\u00f5es \\(h\\) seriam as sa\u00eddas dos n\u00f3s da pen\u00faltima camada. Como nesse caso s\u00f3 temos 2 camadas, essa camada tamb\u00e9m \u00e9 a que recebe os par\u00e2metros do modelo.</p> <p>Para obter o impacto dos par\u00e2metros \\(a\\), \\(b\\), \\(c\\) e \\(d\\) no erro do modelo, precisamos da derivada da fun\u00e7\u00e3o de erro em rela\u00e7\u00e3o a esses par\u00e2metros, ou seja:</p> \\[(\\frac{\u2202g}{\u2202a}, \\frac{\u2202g}{\u2202b}, \\frac{\u2202g}{\u2202c}, \\frac{\u2202g}{\u2202d})\\] <p>Entretanto, essa informa\u00e7\u00e3o n\u00e3o est\u00e1 dispon\u00edvel na \u00faltima camada, uma vez que ela s\u00f3 receber\u00e1 os valores das fun\u00e7\u00f5es \\(h_1\\) e \\(h_2\\) e n\u00e3o o valor dos par\u00e2metros. A derivada calculada na \u00faltima camada seria:</p> \\[(\\frac{\u2202g}{\u2202h_1}, \\frac{\u2202g}{\u2202h_2})\\] <p>Entretanto, podemos continuar calculando a derivada para as camadas anteriores. A derivada na pen\u00faltima camada, em que as sa\u00eddas s\u00e3o \\(h_1\\) e \\(h_2\\) e as entradas s\u00e3o \\(a\\), \\(b\\), \\(c\\) e \\(d\\), seria:</p> \\[(\\frac{\u2202h_1}{\u2202a}, \\frac{\u2202h_1}{\u2202b}, \\frac{\u2202h_2}{\u2202c}, \\frac{\u2202h_2}{\u2202d})\\] <p>Com esses resultados, conseguir\u00edamos relacionar a varia\u00e7\u00e3o da fun\u00e7\u00e3o do erro \\(g\\) com os par\u00e2metros do modelo \\(a\\), \\(b\\), \\(c\\) e \\(d\\), pois pela regra da cadeia temos que:</p> \\[\\frac{\u2202g}{\u2202a} = \\frac{\u2202g}{\u2202h_1}\\cdot\\frac{\u2202h_1}{\u2202a}\\] \\[\\frac{\u2202g}{\u2202b} = \\frac{\u2202g}{\u2202h_1}\\cdot\\frac{\u2202h_1}{\u2202b}\\] \\[\\frac{\u2202g}{\u2202c} = \\frac{\u2202g}{\u2202h_2}\\cdot\\frac{\u2202h_2}{\u2202c}\\] \\[\\frac{\u2202g}{\u2202d} = \\frac{\u2202g}{\u2202h_2}\\cdot\\frac{\u2202h_2}{\u2202d}\\] <p>A propaga\u00e7\u00e3o da derivada far\u00e1 com que a derivada seja calculada em todas as camadas, come\u00e7ando da \u00faltima e indo at\u00e9 a primeira camada, que \u00e9 a que recebe os par\u00e2metros do modelo diretamente. S\u00f3 ent\u00e3o, a informa\u00e7\u00e3o obtida ser\u00e1 suficiente para realizar o ajuste do modelo. Nesse caso, fizemos a derivada 2 vezes pois o exemplo era uma rede de 2 camadas.</p>"},{"location":"optimization/#otimizador","title":"Otimizador","text":"<p>O otimizador \u00e9 uma fun\u00e7\u00e3o que ser\u00e1 utilizada para ajustar as vari\u00e1veis do modelo durante a fase de treinamento, de forma a minimizar o erro do modelo. No nosso modelo estamos utilizando o otimizador Adam, que tamb\u00e9m utiliza o vetor gradiente da fun\u00e7\u00e3o de perda, mas \u00e9 mais complexo do que a simples aplica\u00e7\u00e3o do gradiente descentente e permitir\u00e1 um ajuste mais r\u00e1pido.</p> <p>No ajuste feito pelo Adam, al\u00e9m de tamb\u00e9m ser utilizada a segunda derivada da fun\u00e7\u00e3o de perda, a influ\u00eancia do vetor gradiente \u00e9 ponderada por um fator que decai exponencialmente a cada itera\u00e7\u00e3o, tornando as vari\u00e1veis menos vol\u00e1teis.</p> <p>O ajuste feito pelo Adam, para um modelo com learning rate \u03b1 cujo resultado \u00e9 uma fun\u00e7\u00e3o \\(f(a,b,c)\\) seria como mostrado abaixo:</p> <p>Ter\u00edamos a vari\u00e1vel \\(a\\) ajustada da seguinte forma:</p> \\[a_{otim} = a - \\frac{\u03b1V}{\\sqrt{S}+\u03b5}\\] <p>Em que:</p> \\[V = \\frac{\u03b21S + (1-\u03b21)\\frac{\u2202f}{\u2202a}}{1-{\u03b21}^{t}}\\] \\[S = \\frac{\u03b22S + (1-\u03b22)\\frac{\u2202f}{{\u2202a}^{2}}}{1-{\u03b22}^{t}}\\] <p>Os valores \\(\u03b21\\), \\(\u03b22\\) e \\(\u03b5\\) s\u00e3o constantes, geralmente definida com os seguintes valores:</p> <p>\\(\u03b21 = 0.9\\)</p> <p>\\(\u03b22 = 0.999\\)</p> <p>\\(\u03b5 = {10}^{-8}\\)</p> <p>E \\(t\\) \u00e9 o n\u00famero de itera\u00e7\u00f5es j\u00e1 feitas no modelo.</p> <p>C\u00f3digo: Otimiza\u00e7\u00e3o do modelo</p> <p>Um exemplo simples da utiliza\u00e7\u00e3o do otimizador pode ser visto abaixo:</p> <p>OBS.: Esse c\u00f3digo foi bastante simplificado, a implementa\u00e7\u00e3o real \u00e9 um pouco diferente. <pre><code>model = ECNN() # iniciando o modelo\ncriterion = nn.MSELoss() # iniciando a fun\u00e7\u00e3o de perda\n# iniciando o otimizador para os par\u00eamtros do modelo e com o learning rate desejado\noptimizer = Adam(ecnn.parameters(), lr=learning_rate) \n\n\nfor epoch in range(epochs): # para cada epoch...\n    for i, batch in enumerate(dataloader): # para cada batch...\n        gray, color, category = batch # extraindo os dados da batch\n\n        optimizer.zero_grad() # zerando os gradientes do otimizador\n        outputs = ecnn(gray, category) # passando os dados pelo modelo\n\n        loss = criterion(outputs, color) # calculando a perda\n        loss.backward() # derivando a perda\n        optimizer.step()\n</code></pre></p>"},{"location":"optimization/#referencias","title":"Refer\u00eancias","text":"<ol> <li>What is Adam Optimizer?</li> <li>Optimization techniques for Gradient Descent</li> <li>Exponential smoothing</li> <li>PyTorch: Connection Between loss.backward() and optimizer.step()</li> <li>Gradient Descent in Linear Regression</li> </ol>"},{"location":"testing/","title":"Testando o Modelo","text":"<p>Ap\u00f3s o treinamento do modelo, devemos testar nosso modelo em um conjunto de imagem completamente novo para sabermos se ele \u00e9 capaz de generalizar bem a coloriza\u00e7\u00e3o para imagens que ele nunca viu antes. Para isso, utilizaremos o conjunto de teste que foi separado anteriormente.</p> <p>Nosso modelo treinado com <code>50 Epochs</code>, <code>5000 imagens</code> e um learning rate de <code>0.01</code> foi capaz de atingir um <code>MSE</code> de \\(142.98\\) no conjunto de treino e \\(174,64\\) no conjunto de teste </p> <p></p>"},{"location":"unet/","title":"U-Net Layout","text":"<p><code>U-Net</code> \u00e9 um tipo especial de Rede Neural Convolucional (CNN) criada especificamente para o uso em imagens. Ela \u00e9 composta de duas etapas, uma de Encoding e outra de Decoding, semelhante a estrutura do padr\u00e3o do Autoencoder, por\u00e9m criando um formato de U (por isso U-Net) e diversas conex\u00f5es atalho. Assim como no Autoecoder, a estrutura ser\u00e1 divididas em diferentes n\u00edveis, e a etapa de Encoding normalmente possui o mesmo n\u00famero de n\u00edveis que a etapa de Decoding, as vezes possuindo, tamb\u00e9m, um ou mais n\u00edveis de transi\u00e7\u00e3o.</p> <p></p> <p>Refer\u00eancia: Example Arquiteture of U-Net</p> <p>Na etapa de Encoding a imagem ir\u00e1 realizar uma decida passando por meio de seus n\u00edveis, em cada n\u00edvel a imagem ter\u00e1 suas dimens\u00f5es diminu\u00eddas, por\u00e9m a quantidade de layers aumentada. J\u00e1 na etapa de Decoding, a imagem realizar\u00e1 uma subida e, a cada n\u00edvel, suas dimens\u00f5es ser\u00e3o aumentadas e sua quantidade de layers diminu\u00edda (seguindo a mesma propor\u00e7\u00e3o da etapa de Encoding) at\u00e9 chegar nas dimens\u00f5es originais e a quantidade de layers desejada. </p> <p>Al\u00e9m disso, por meio das conex\u00f5es atalho, durante cada n\u00edvel do Decoding haver\u00e1 a concatena\u00e7\u00e3o da imagem atual com a imagem do respectivo n\u00edvel da etapa de Encoding, a imagem com mesmas dimens\u00f5es e quantidade de layers.</p> <p>Essas conex\u00f5es atalho s\u00e3o utilizadas para enviar imagens diretamente da etapa de Encoding para a etapa de Decoding sem que elas precisem passar por todos os n\u00edveis. Isso permite que recursos de alto e baixa complexidade sejam preservados e aprendidos, por meio da redu\u00e7\u00e3o do problema do desaparecimento de gradientes, reduzindo, assim, a perda de informa\u00e7\u00f5es que pode ocorrer durante a etapa de Encoding.</p>"},{"location":"unet/#fowarding","title":"Fowarding","text":"<p>Antes de construirmos nosso primeiro modelo utilizando os conhecimentos obtidos at\u00e9 aqui precisamos entender um \u00faltimo conceito.</p> <p><code>`Forwarding</code>, <code>Forward Pass</code> ou <code>Propaga\u00e7\u00e3o Direta</code> \u00e9 o processo pelo qual uma rede neural processa uma entrada e produz uma sa\u00edda. Nele uma entrada passa sequencialmente por cada camada do modelo e, ao fim de todas elas, produz um resultado final.</p> <p>Iremos implement\u00e1-lo nos nossos modelos por meio da defini\u00e7\u00e3o da fun\u00e7\u00e3o <code>forward</code> que ser\u00e1 chamada a cada execu\u00e7\u00e3o do modelo.</p>"},{"location":"unet/#construindo-um-modelo","title":"Construindo um Modelo","text":"<p>Construiremos o c\u00f3digo de um modelo simples, por\u00e9m completo, que utiliza o U-Net Layout.</p> <p>Nesse modelo teremos dois n\u00edveis com  camadas <code>convolucionais</code> no Encoder, uma camada de <code>dilata\u00e7\u00e3o</code> no n\u00edvel de transi\u00e7\u00e3o, e dois n\u00edveis com camadas <code>convolucionais transpostas</code> no Decoder. Todas as camadas ser\u00e3o seguidas de uma camada de <code>Normaliza\u00e7\u00e3o em Batches</code>, menos no ultimo n\u00edvel do Decoder.</p> <p>Declara\u00e7\u00e3o das Camadas<pre><code># Encoder\n# entrada: (1, w, h) -&gt; saida: (32, w/2, h/2)\nconv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1) \nconv1_bn = nn.BatchNorm2d(32)\n\n# entrada: (32, w/2, h/2) -&gt; saida: (64, w/4, h/4)\nconv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\nconv2_bn = nn.BatchNorm2d(64)\n\n# N\u00edvel de Transi\u00e7\u00e3o\n# entrada: (64, w/4, h/4) -&gt; saida: (128, w/8, h/8)\nconv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\nconv3_bn = nn.BatchNorm2d(128)\n\n# Decoder\n# entrada: (128, w/8, h/8) -&gt; saida: (64, w/4, h/4)\ntconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\ntconv2_bn = nn.BatchNorm2d(64)\n\n# entrada: (128, w/4, h/4) -&gt; saida: (32, w/2, h/2)\ntconv1 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\ntconv1_bn = nn.BatchNorm2d(32)\n\n# entrada: (64, w/2, h/2) -&gt; saida: (2, w, h)\ntconv0 = nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1)\n\n# entrada: (3, w, h) -&gt; saida: (2, w, h)\ntconv_out = nn.ConvTranspose2d(3, 2, kernel_size=3, stride=1, padding=1)\n</code></pre> Observe que a quantidade de layers das imagens que entram no <code>tconv1</code> e <code>tconv0</code> \u00e9 o dobro da quantidade de layer que sai da <code>tconv2</code> e <code>tconv1</code>, respectivamente. Isso ocorre devido a concatena\u00e7\u00e3o das imagens que saem dessas camadas com as imagens vindas de suas respectivas conex\u00f5es atalho. </p> <p>Al\u00e9m disso, note a presen\u00e7a de uma camada de convolu\u00e7\u00e3o transposta de sa\u00edda que servir\u00e1 ajustar a quantidade de layers que ficou diferente do objetivo (duas: A+B) ap\u00f3s a concatena\u00e7\u00e3o das imagens do n\u00edvel 0 (imagem em escala de cinza). Por fim, observe tamb\u00e9m que nos dois \u00faltimos n\u00edveis do Decoder n\u00e3o h\u00e1 \u00e0 aplica\u00e7\u00e3o de camadas de normaliza\u00e7\u00e3o em batches. Essa escolha foi tomada devido a baixa efetividade da normaliza\u00e7\u00e3o em batches em poucas quantidades de layers.</p> <p>A decida por cada n\u00edvel da etapa de Encoding consistir\u00e1 em aplicar a camada convolucional, seguida da camada de normaliza\u00e7\u00e3o em batches e por fim aplica\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU.  Fun\u00e7\u00e3o de Forwarding (Encoder)<pre><code>gray_conv1 = relu(self.conv1_bn(self.conv1(gray))) # N\u00edvel 1\ngray_conv2 = relu(self.conv2_bn(self.conv2(gray_conv1))) # N\u00edvel 2\n\ngray_conv3 = relu(self.conv3_bn(self.conv3(gray_conv2))) # N\u00edvel de Transi\u00e7\u00e3o\n</code></pre></p> <p>J\u00e1 a subida por cada n\u00edvel da etapa de Decoding consistir\u00e1 em aplicar a camada convolucional transposta, seguida da camada de normaliza\u00e7\u00e3o em batches, aplicar a fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU e por fim concatenar o resultado com a imagem resultade do respectivo n\u00edvel da etapa de Encoding. Por\u00e9m h\u00e1 uma pequena  Fun\u00e7\u00e3o de Forwarding (Decoder)<pre><code>gray_tconv2 = relu(self.tconv2_bn(self.tconv2(gray_conv3)))\ngray_tconv2 = cat((gray_tconv2, gray_conv2), 1)\ngray_tconv1 = relu(self.tconv1(gray_tconv2))\ngray_tconv1 = cat((gray_tconv1, gray_conv1), 1)\n\ngray_tconv0 = relu(self.tconv0(gray_tconv1))\ngray_tconv0 = cat((gray_tconv0, gray), 1)\n\noutput = self.tconv_out(gray_tconv0)\n</code></pre></p> Modelo Completo <p>Para criar um modelo completo e funcional, encapsularemos as duas partes acima em um classe: Modelo Completo<pre><code>from torch import nn, Tensor, cat\nfrom torch.nn.functional import relu\n\nclass UnetModel(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n        # Encoder\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n        self.conv1_bn = nn.BatchNorm2d(32)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n        self.conv2_bn = nn.BatchNorm2d(64)\n\n        # N\u00edvel de Transi\u00e7\u00e3o\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n        self.conv3_bn = nn.BatchNorm2d(128)\n\n        # Decoder\n        self.tconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n        self.tconv2_bn = nn.BatchNorm2d(64)\n\n        self.tconv1 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n        self.tconv1_bn = nn.BatchNorm2d(32)\n\n        self.tconv0 = nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1)\n\n        self.tconv_out = nn.ConvTranspose2d(3, 2, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, gray: Tensor) -&gt; Tensor:\n\n        # Encoder\n        gray_conv1 = relu(self.conv1_bn(self.conv1(gray)))\n        gray_conv2 = relu(self.conv2_bn(self.conv2(gray_conv1)))\n\n        # N\u00edvel de Transi\u00e7\u00e3o\n        gray_conv3 = relu(self.conv3_bn(self.conv3(gray_conv2)))\n\n        # Decoder\n        gray_tconv2 = relu(self.tconv2_bn(self.tconv2(gray_conv3)))\n        gray_tconv2 = cat((gray_tconv2, gray_conv2), 1)\n\n        gray_tconv1 = relu(self.tconv1(gray_tconv2))\n        gray_tconv1 = cat((gray_tconv1, gray_conv1), 1)\n\n        gray_tconv0 = relu(self.tconv0(gray_tconv1))\n        gray_tconv0 = cat((gray_tconv0, gray), 1)\n\n        output = self.tconv_out(gray_tconv0)\n        return output\n</code></pre></p>"},{"location":"unet/#referencias","title":"Refer\u00eancias:","text":"<ol> <li>A U-Net: A Complete Guide</li> </ol>"}]}