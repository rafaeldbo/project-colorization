{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_start\"></a>\n",
    "# **Embedded Convolutional Neural Network - Tutorial Prático**\n",
    "\n",
    "Esta é a parte prática do tutorial de construção de um modelo de colorização de imagens classificadas. Nessa `Notebook`, você será guiado a construir código do modelo **ECNN** e também os códigos necessários para **treinar** o modelo, **testar** sua acurácia e **aplicar** o modelo para colorir novas imagens.\n",
    "\n",
    "Esse tutorial possuirá pouca teoria, espera-se você já tenha lido a parte teórica nessecessária para entender o que será feito aqui presente em nosso [site](https://rafaeldbo.github.io/project-colorization/context). De qualquer maneira, as sessões necessárias para melhor entendimento de cada etapa desse `Notebook` serão referênciadas para consulta na própria etapa. \n",
    "\n",
    "## **Sumário**\n",
    "\n",
    "1. [Preparando as Imagens](#go_to_load_images)\n",
    "2. [Construindo o Modelo](#go_to_building_model)\n",
    "\n",
    "    2.1. [Criando a Estrutura do Modelo](#go_to_model_layers)\n",
    "\n",
    "    2.2. [Função \"forward\"](#go_to_forward)\n",
    "    \n",
    "    2.3. [Modelo Completo](#go_to_ECNN_model)\n",
    "    \n",
    "3. [Construindo a Rotina de Treinamento](#go_to_train_model)\n",
    "4. [Construindo a Rotina de Testes](#go_to_test_model)\n",
    "5. [Aplicando o Modelo](#go_to_deploy_model)\n",
    "\n",
    "## **Instalando Dependências**\n",
    " \n",
    "Antes de começarmos, caso não tenha as bibliotecas necessárias, rode a célula abaixo para instala-las ou instale utilizando o arquivo [requirements.txt](https://github.com/rafaeldbo/project-colorization/blob/main/requirements.txt) presente em nosso repositório. É recomendado o uso de um ambiente virtual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só é necessário rodar esse script uma vez para instalar as dependências necessárias\n",
    "!python -m pip install torch scikit-image matplotlib pandas numpy jupyter ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso possua **GPU** com suporte ao `cuda` e deseja utilizá-la, verifique se ela já está disponivel utilizando o código abaixo. Caso não esteja, acesse esse [site](https://pytorch.org/get-started/locally/) e escolha a versão do `pytorch` com maior compatibilidade com sua GPU e a instale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"versão do torch: {torch.__version__}\")\n",
    "print(\"cuda disponivel!\" if torch.cuda.is_available() else \"cuda indisponível :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importando as bibliotecas necessárias\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path, listdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor, no_grad, cat, from_numpy, save, load\n",
    "from torch.nn.functional import relu\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2lab, lab2rgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_load_images\"></a>\n",
    "## **Preparando as Imagens**\n",
    "\n",
    "A teoria referente a essa etapa está disponível na sessão [Entradas e Saídas do Modelo](https://rafaeldbo.github.io/project-colorization/inputs_outputs) do nosso site.\n",
    "\n",
    "Um ponto importante do modelo que criaremos nesse tutorial é que, após ser treinado por um conjunto de imagens de determinadas dimensões $(largura \\times altura)$, ele só poderá ser aplicado em imagens destas mesmas dimensões. Nesse tutorial, utilizaremos as imagens do dataset [Image Colorization Dataset](https://www.kaggle.com/datasets/aayush9753/image-colorization-dataset) que possuem as dimensões $400 \\times 400$, dessa forma, todas as imagens recebidas e \"geradas\" pelo modelo possuirão essas mesmas dimensões. Além disso, como se trata de um modelo que colore imagem categorizadas, também precisaremos de um arquivos com as categorias de cada imagem. Felizmente nosso time já preparou o aquivo [categories.csv](https://drive.google.com/uc?export=download&id=115OMNGbthQ5CFmnvPUlxYZ-_Y1CNxI9b) exatamente com essa informação. Coloque tanto as pastas de imagens do dataset quanto o arquivo de categorias em uma pasta `data`, para melhor organização.\n",
    "\n",
    "Para preparar as imagens para a utilização pelo nosso modelo precisaremos criar um classe de **dataset** personalizada baseada na classe `Dataset` do `pytorch`. Ela deverá localizar todas a imagens que serão utilizadas pelo modelo e possuir uma função `__getitem__` que  caregará uma imagem de cada vez, fornecendo os dados da imagem e sua categoria. Essa estrutura será importante para que, no futuro, o `DataLoader` seja capaz de utilizar essa classe **dataset** para carregar as imagens dos nossos **Batches** (caso não se lembre do que estamos falando, isso será retomado mais adiante). Para facilitar, já iremos fazer com que ela retorne separadamente o *layer* **L** e os *layers* **AB**.\n",
    "\n",
    "O código desse dataset personalizado será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "        images_path: str, # Caminho da pasta onde estão as imagens \n",
    "        categories_file: str, # Caminho do arquivo csv com as categorias das imagens\n",
    "        size: int = -1, # Quantidade de imagens a serem carregadas, sendo -1 para todas\n",
    "    ):\n",
    "        # criando uma lista com os arquivos das imagens\n",
    "        self.images_path = images_path\n",
    "        images = listdir(images_path) # listando os arquivos da pasta\n",
    "        size = size if size > 0 else len(images)\n",
    "        self.images_files = images[:size]\n",
    "        \n",
    "        # criando um dicionário com as categorias das imagens\n",
    "        df = pd.read_csv(path.join(categories_file), delimiter=';') # lendo o arquivo csv\n",
    "        df['category'] = df['category'].fillna(0) # colocando a categoria 0 para as imagens sem categoria, caso existam\n",
    "        self.categories = df.set_index('image')['category'].to_dict() # criando o dicionário\n",
    "\n",
    "    # função que retorna o tamanho do dataset\n",
    "    def __len__(self):\n",
    "        return len(self.images_files)\n",
    "\n",
    "    def __getitem__(self, \n",
    "        index: int, # Índice da imagem a ser carregada\n",
    "    ) -> tuple[Tensor, Tensor, int]:\n",
    "        \n",
    "            # lendo a imagem\n",
    "            img_file = self.images_files[index]\n",
    "            img_path = path.join(self.images_path, img_file)\n",
    "            img = imread(img_path) \n",
    "\n",
    "            # converte a imagem de RGB para LAB\n",
    "            LAB_img = from_numpy(rgb2lab(img)) \n",
    "            LAB_img = LAB_img.permute(2, 0, 1) \n",
    "\n",
    "            # separa os layers\n",
    "            gray_layer = LAB_img[0, :, :].unsqueeze(0) \n",
    "            color_layers = LAB_img[1:, :, :] \n",
    "            \n",
    "            # retornando o layer L, os layers AB e a categoria da imagem\n",
    "            return gray_layer.float(), color_layers.float(), self.categories.get(img_file, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=go_to_building_model> </a>\n",
    "## **Construindo o Modelo**\n",
    "\n",
    "A teoria referente a essa etapa está disponível em diversas sessões do site, começando pela sessão [Tipo do Modelo](https://rafaeldbo.github.io/project-colorization/model_type). Nosso modelo consistirá em um **Autoencoder** formado por **redes neurais convolucionais** (**CNNs**) e camadas de *embeddings* utilizando um layout `U-Net`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=go_to_model_layers></a>\n",
    "### **Criando a Estrutura do Modelo**\n",
    "\n",
    "Na parte convolucional da U-net (consulte as sessões [Tipos de Convoluções](https://rafaeldbo.github.io/project-colorization/convolutions) e [U-Net](https://rafaeldbo.github.io/project-colorization/unet) para saber mais), teremos:\n",
    "\n",
    "- 4 níveis nas etapas de **Encoding** e **Decoding**\n",
    "\n",
    "| `Nível`   | **Encoder**      | **Decoder**                     |\n",
    "|-----------|------------------|---------------------------------|\n",
    "| `Nível 0` | 1 layer $^{(1)}$ | 2 layer &  1 layer $^{(1, 2)}$  |\n",
    "| `Nível 1` | 32 layers        | 32 layers & 32 layers $^{(2)}$  | \n",
    "| `Nível 2` | 64 layers        | 64 layers & 64 layers $^{(2)}$  |\n",
    "| `Nível 3` | 128 layers       | 128 layer & 128 layers $^{(2)}$ |\n",
    "\n",
    "**OBS¹.:** esse \"1 layer\" se refere a *layer* de entrada (camada L da imagem).\n",
    "\n",
    "**OBS².:** utilizamos o símbolo $\\&$ para indicar concatenação dos *layers*.\n",
    "\n",
    "**OBS³.:** os quantidade de *layers* mostrados na tabela indicam a quantidade de *layers* na saída de nível de convolução. \n",
    "\n",
    "- 3 camadas de transição\n",
    "    - Uma convolucional simples de entrada 128 *layers* e saida 256 *layers*\n",
    "    - duas convolucionais de dilatação que não alteram as dimensões ou os *layers*\n",
    "\n",
    "- 1 camada convolucional de saida de entrada 3 *layers* (nível 0 do Decoder) e sáida 2 *layers* (objetivo)\n",
    "\n",
    "- Utilizaremos uma camada de **normalização em batches** após cada camada convolucional, menos no `Nível 0` do **Decoder** e na camada de saída (consulte o tópico [Normalização em Batches](https://rafaeldbo.github.io/project-colorization/batnorm_actfunc/#normalizacao-em-batches-batch-normalization) para saber mais).\n",
    "\n",
    "Além disso, para acrescentar as informações da categoria no processo utilizaremos uma camada de `embeddings` (consulte a sessão [Camada de Embeddings](https://rafaeldbo.github.io/project-colorization/embeddings/) para saber mais) que será construída utilizando um vetor de 10 caraterísticas e as 8 categorias que possuímos (as `7` que foram usadas para classificar as imagens mais a categoria `0` para as imagens sem categoria). Essa camada será conectada aos *layers* do `Nível 0` do **Encoder** e o `Nível 3` do **Decoder** por meio da concatenação dos *layers* da \"imagem\" com os *layers* formados pelo vetor de característiscas da categoria (*embbeding*). Não se preocupe se não entendeu, isso ficará mais claro quando aplicarmos durante a construção da função de `fowarding` do modelo.\n",
    "\n",
    "``` python\n",
    "# Embeddings\n",
    "emb_size = 10\n",
    "embd = nn.Embedding(8, emb_size)\n",
    "\n",
    "# Encoder\n",
    "# Nível 1 (1+emb_size -> 32)\n",
    "conv1 = nn.Conv2d(1 + emb_size, 32, kernel_size=4, stride=2, padding=1)\n",
    "conv1_bn = nn.BatchNorm2d(32) \n",
    "\n",
    "# Nível 2 (32 -> 64)\n",
    "conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "conv2_bn = nn.BatchNorm2d(64)\n",
    "\n",
    "# Nível 3 (64 -> 128)\n",
    "conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "conv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "# Transição\n",
    "# Convolução de transição (128 -> 256)\n",
    "conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "conv4_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "# Primeira dilatação\n",
    "dilat1 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "dilat1_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "# Segunda dilatação\n",
    "dilat2 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "dilat2_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "# Decoder\n",
    "# Nível 3 (256+emb_size -> 128)\n",
    "tconv3 = nn.ConvTranspose2d(256 + emb_size, 128, kernel_size=4, stride=2, padding=1)\n",
    "tconv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "# Nível 2 (128+128 -> 64)\n",
    "tconv2 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "tconv2_bn = nn.BatchNorm2d(64)\n",
    "\n",
    "# Nível 1 (64+64 -> 32)\n",
    "tconv1 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "tconv1_bn = nn.BatchNorm2d(32)\n",
    "\n",
    "# Nível 0 (32+32 -> 2)\n",
    "tconv0 = nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "# Saída (2+1 -> 2)\n",
    "tconv_out = nn.Conv2d(3, 2, kernel_size=3, stride=1, padding=1)\n",
    "```\n",
    "\n",
    "Note que como iremos concatenar as informações do *embedding* com as informações da imagem, a entrada das camadas convolucionais iniciais do **Encoder** e **Decoder** terão um número de *layers* maior que o normal (será somado a tamanho do *embedding*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=go_to_forward> </a>\n",
    "## **Função `forward`**\n",
    "\n",
    "A função `forward` é a função que será chamada quando passarmos uma imagem pelo modelo. Ela é responsável por passar a imagem por todas as camadas do modelo e retornar a imagem colorida (consulte [Forwarding](https://rafaeldbo.github.io/project-colorization/unet/#fowarding) para saber mais).\n",
    "\n",
    "Antes de implementá-la, precisamos entender como será feita a concatenação das informações do *embedding* com as informações da imagem. Faremos isso em 4 passos:\n",
    "\n",
    "1. **Embedding da Categoria**: a categoria da imagem será passada por uma camada de *embeddings* que transformará a categoria em um vetor de 10 características. \n",
    "2. **Reorganização do Embedding**: reorganizaremos o vetor de características para poder ser aplicado a um **batch** de imagens. Isso será feito adicionando mais uma dimensão ao vetor.\n",
    "3. **Replicação do Embedding**: o vetor de características será replicado para o tamanho da imagem. Isso será feito para que o vetor de características possa ser concatenado com a imagem.\n",
    "4. **Concatenação**: por fim, concatenaremos o vetor de características formatado com a imagem.\n",
    "\n",
    "O código responsável por fazer essas 4 etapas é:\n",
    "\n",
    "```python\n",
    "embd_category = embd(category) # obtendo o vetor de características\n",
    "embd_category = embd_category.view(-1, emb_size, 1, 1) # colocando a dimensão extra\n",
    "embd_category = embd_category.repeat(1, 1, img.shape[2], img.shape[3]) # replicando o vetor\n",
    "\n",
    "img_embd = cat((img, embd_category), 1) # concatenando a imagem com o vetor formatado\n",
    "```\n",
    "\n",
    "Agora que entendemos como será feita a concatenação, podemos implementar a função `forward` do modelo. Nele, nós utilizaremos o `ReLU` como função de ativação (consulte o tópico [Funções de Ativação](https://rafaeldbo.github.io/project-colorization/batnorm_actfunc/#funcoes-de-ativacao-activation-functions) para saber mais). Os níveis serão construidos da seguinte forma:\n",
    "- **Encoder e Transição**\n",
    "    - Aplicação da camada de `convolução`;\n",
    "    - Aplicação da camada de `normalização em batches`;\n",
    "    - Aplicação da função de ativação `ReLU`.\n",
    "- **Decoder**\n",
    "    - Aplicação da camada de `convolução transposta`;\n",
    "    - Aplicação da camada de `normalização em batches`;\n",
    "    - Aplicação da função de ativação `ReLU`;\n",
    "    - Concatenação com a saída do nível correspondente do **Encoder**.\n",
    "\n",
    "Obtedno a seguinte estrutura:\n",
    "``` python\n",
    "def forward(gray, category):\n",
    "    \n",
    "    # Primeiro Embedding\n",
    "    # Obtendo o vetor de características formatado para a concatenação com a imagem de entrada\n",
    "    embd_decoder = embd(category)\\ \n",
    "        .view(-1, emb_size, 1, 1)\\\n",
    "        .repeat(1, 1, gray.shape[2], gray.shape[3])\n",
    "    \n",
    "    # Concatenação da imagem de entrada com o vetor de características\n",
    "    gray_embd_encoder = cat((gray, embd_decoder), 1)\n",
    "\n",
    "    # Encoder\n",
    "    gray_conv1 = relu(conv1_bn(conv1(gray_embd_encoder))) # Nível 1\n",
    "    gray_conv2 = relu(conv2_bn(conv2(gray_conv1))) # Nível 2\n",
    "    gray_conv3 = relu(conv3_bn(conv3(gray_conv2))) # Nível 3\n",
    "\n",
    "    # Transição\n",
    "    gray_conv4 = relu(conv4_bn(conv4(gray_conv3))) # convolução de transição\n",
    "    gray_dilat1 = relu(dilat1_bn(dilat1(gray_conv4))) # primeira dilatação\n",
    "    gray_dilat2 = relu(dilat2_bn(dilat2(gray_dilat1))) # segunda dilatação\n",
    "\n",
    "    # Segundo Embedding\n",
    "    # Obtendo o vetor de características formatado para a concatenação com de entrada no Decoder\n",
    "    embd_decoder = embd(category)\\\n",
    "        .view(-1, emb_size, 1, 1)\\\n",
    "        .repeat(1, 1, gray_dilat2.shape[2], gray_dilat2.shape[3])\n",
    "        \n",
    "    # Concatenação da imagem de entrada do decoder com o vetor de características\n",
    "    gray_embd_decoder = cat((gray_dilat2, embd_decoder), 1) \n",
    "\n",
    "    # Decoder\n",
    "    gray_tconv3 = relu(tconv3_bn(tconv3(gray_embd_decoder))) # Nível 3\n",
    "    gray_tconv3 = cat((gray_tconv3, gray_conv3), 1) # concatenação com a saída do nível 3 do Encoder\n",
    "    gray_tconv2 = relu(tconv2_bn(tconv2(gray_tconv3))) # Nível 2\n",
    "    gray_tconv2 = cat((gray_tconv2, gray_conv2), 1) # concatenação com a saída do nível 2 do Encoder\n",
    "    gray_tconv1 = relu(tconv1_bn(tconv1(gray_tconv2))) # Nível 1\n",
    "    gray_tconv1 = cat((gray_tconv1, gray_conv1), 1) # concatenação com a saída do nível 1 do Encoder\n",
    "    gray_tconv0 = relu(tconv0(gray_tconv1)) # Nível 0\n",
    "    gray_tconv0 = cat((gray_tconv0, gray), 1) # concatenação com a imagem de entrada\n",
    "\n",
    "    output = tconv_out(gray_tconv0) # gerando as camadas AB de saída\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_ECNN_model\"> </a>\n",
    "### **Modelo Completo**\n",
    "\n",
    "Agora que temos a estrutura do modelo, podemos construir a classe do modelo. Ela será construída utilizando a classe `nn.Module` do `pytorch` como base, terá a função `forward` que acabamos de construir. Além disso, ela terá uma função `__init__` que inicializará todas as camadas do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECNN_model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.emb_size = 10\n",
    "        self.embd = nn.Embedding(8, self.emb_size)\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1 + self.emb_size, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Transition\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(256)\n",
    "        self.dilat1 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "        self.dilat1_bn = nn.BatchNorm2d(256)\n",
    "        self.dilat2 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "        self.dilat2_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Decoder\n",
    "        self.tconv3 = nn.ConvTranspose2d(256 + self.emb_size, 128, kernel_size=4, stride=2, padding=1 )\n",
    "        self.tconv3_bn = nn.BatchNorm2d(128)\n",
    "        self.tconv2 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.tconv2_bn = nn.BatchNorm2d(64)\n",
    "        self.tconv1 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.tconv1_bn = nn.BatchNorm2d(32)\n",
    "        self.tconv0 = nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.tconv_out = nn.Conv2d(3, 2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        gray: Tensor,\n",
    "        category: int,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        # First Embeeding\n",
    "        embd_decoder = self.embd(category)\\\n",
    "            .view(-1, self.emb_size, 1, 1)\\\n",
    "            .repeat(1, 1, gray.shape[2], gray.shape[3])\n",
    "        gray_embd_encoder = cat((gray, embd_decoder), 1)\n",
    "\n",
    "        # Encoder\n",
    "        gray_conv1 = relu(self.conv1_bn(self.conv1(gray_embd_encoder)))\n",
    "        gray_conv2 = relu(self.conv2_bn(self.conv2(gray_conv1)))\n",
    "        gray_conv3 = relu(self.conv3_bn(self.conv3(gray_conv2)))\n",
    "\n",
    "        # Transition\n",
    "        gray_conv4 = relu(self.conv4_bn(self.conv4(gray_conv3)))\n",
    "        gray_dilat1 = relu(self.dilat1_bn(self.dilat1(gray_conv4)))\n",
    "        gray_dilat2 = relu(self.dilat2_bn(self.dilat2(gray_dilat1)))\n",
    "\n",
    "        # Second Embeeding\n",
    "        embd_decoder = self.embd(category)\\\n",
    "            .view(-1, self.emb_size, 1, 1)\\\n",
    "            .repeat(1, 1, gray_dilat2.shape[2], gray_dilat2.shape[3])\n",
    "        gray_embd_decoder = cat((gray_dilat2, embd_decoder), 1)\n",
    "\n",
    "        # Decoder\n",
    "        gray_tconv3 = relu(self.tconv3_bn(self.tconv3(gray_embd_decoder)))\n",
    "        gray_tconv3 = cat((gray_tconv3, gray_conv3), 1)\n",
    "        gray_tconv2 = relu(self.tconv2_bn(self.tconv2(gray_tconv3)))\n",
    "        gray_tconv2 = cat((gray_tconv2, gray_conv2), 1)\n",
    "        gray_tconv1 = relu(self.tconv1_bn(self.tconv1(gray_tconv2)))\n",
    "        gray_tconv1 = cat((gray_tconv1, gray_conv1), 1)\n",
    "        gray_tconv0 = relu(self.tconv0(gray_tconv1))\n",
    "        gray_tconv0 = cat((gray_tconv0, gray), 1)\n",
    "\n",
    "        output = self.tconv_out(gray_tconv0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_train_model\"> </a>\n",
    "## **Construindo a Rotina de Treinamento**\n",
    "\n",
    "A teoria referente a essa etapa está disponível em diversas sessões do site, começando pela sessão [Otimização dos Parâmetros do Modelo](https://rafaeldbo.github.io/project-colorization/optimization). Para treinar um modelo é preciso definir uma função de perda, que irá mensurar os erros do nosso modelo, e um otimizador, que irá corrigir os parâmetros do modelo com base no erro encontrado (consulte os tópicos [Funções de Perda](https://rafaeldbo.github.io/project-colorization/optimization/#funcao-de-perda) e [Otimizador](https://rafaeldbo.github.io/project-colorization/optimization/#otimizador) para saber mais). A função de perda que usaremos será a `MSE` (Mean Squared Error) e o otimizador será o `Adam`.\n",
    "\n",
    "Para treinar o modelo, utilizaremos um `DataLoader` que carregará os dados do nosso dataset e os dividirá em **batches**. Cada **batch** será passado pelo modelo e, e em seguida, passará pela *função de perda* para mensurar os erros. O otimizador então corrigirá os parâmetros do modelo com base na função de perda. Esse processo será repetido por um número de **epochs** definido por nós (consulte [Batches e Epochs](https://rafaeldbo.github.io/project-colorization/batches_epochs) para saber mais).\n",
    "\n",
    "Por fim, salvaremos o modelo treinado para que possamos utilizá-lo posteriormente.\n",
    "\n",
    "**OBS.:** Nesse tutorial, não utilizaremos **multiprocessamento**, o que agilizaria o processo de treino, pois isso demandaria uma estrutura de código mais robusta, que não é a proposta desse tutorial. Caso queira saber mais sobre **multiprocessamento**, consulte a sessão [Multiprocessamento](https://rafaeldbo.github.io/project-colorization/multiprocessing) do nosso site. O código principal que elaboramos, disponível em nosso [repositório](https://github.com/rafaeldbo/project-colorization/tree/main/code), já possui essa funcionalidade, caso queira consultar. \n",
    "\n",
    "Antes de construir a rotina de treinamento, vamos definir alguns parâmetros que serão utilizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_images = 64 # quantidade de imagens a serem usados no treinamento\n",
    "n_test_images = 100 # quantidade de imagens a serem usadas no teste\n",
    "\n",
    "batch_size = 32 # quantidade de imagens por batch\n",
    "epochs = 50 # quantidade de Epochs de treinamento\n",
    "learning_rate = 0.001 # taxa de aprendizado\n",
    "\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "actual_dir = path.dirname(path.abspath(__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS.:** Por padrão, tentaremos treinar e testar o modelo utilizando a `cuda` da **GPU** (caso esteja disponível). Caso não queria utilizar a `cuda`, basta fixar a variável `device` em `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(\n",
    "     images_path = path.join(actual_dir, \"data\", \"train_color\"), # caminho para a pasta com as imagens coloridas\n",
    "     categories_file = path.join(actual_dir, \"data\", \"categories.csv\"), # caminho para o arquivo csv com as categorias das imagens\n",
    "     size = n_train_images # quantidade de imagens a serem carregadas\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size, # tamanho dos batches que serão gerados\n",
    "    shuffle=True # os batches serão formados de maneira aleatória\n",
    ")\n",
    "\n",
    "ecnn = ECNN_model().to(device) # iniciando o modelo\n",
    "criterion = nn.MSELoss() # iniciando a função de perda\n",
    "optimizer = Adam(ecnn.parameters(), lr=learning_rate) # iniciando o otimizador para os parêmtros do modelo e com o learning rate desejado\n",
    "\n",
    "running_losses = [] # lista para armazenar as perdas de cada epoch\n",
    "\n",
    "print(f\"Número de Parâmetros Treináveis: {sum(p.numel() for p in ecnn.parameters())}\")\n",
    "\n",
    "# Criando a barra de progresso\n",
    "total_batches = epochs * len(dataloader)\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Progresso do Treinamento\", position=0)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs): # para cada epoch...\n",
    "    epoch_running_loss = 0\n",
    "    for i, batch in enumerate(dataloader): # para cada batch...\n",
    "        gray, color, category = batch # extraindo os dados da batch\n",
    "\n",
    "        # movendo os dados para o dispositivo de treinamento\n",
    "        gray = gray.to(device) \n",
    "        color = color.to(device)\n",
    "        category = category.to(device)\n",
    "    \n",
    "        optimizer.zero_grad() # zerando os gradientes do otimizador\n",
    "        outputs = ecnn(gray, category) # passando os dados pelo modelo\n",
    "\n",
    "        loss = criterion(outputs, color) # calculando a perda\n",
    "        loss.backward() # derivando a perda\n",
    "        optimizer.step() # atualizando os parâmetros do modelo com base na perda\n",
    "\n",
    "        epoch_running_loss += loss.item()\n",
    "        \n",
    "        # Atualizando a barra de progresso\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(epoch=epoch+1, batch=i+1, loss=round(loss.item()/len(batch), 2))\n",
    "        \n",
    "    running_losses.append(epoch_running_loss)\n",
    "    \n",
    "progress_bar.close()\n",
    "print(f\"Treinamento terminado!\")\n",
    "\n",
    "# Salvando o modelo\n",
    "save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": ecnn.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"time\": time.time() - start,\n",
    "        \"running_losses\": running_losses\n",
    "    }, \n",
    "    path.join(actual_dir, \"ecnn_model.pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_test_model\"> </a>\n",
    "## **Construindo a Rotina de Testes**\n",
    "\n",
    "O método que utilizaremos para testar o modelo é bem simples, usaremos o **modelo treinado** para colorir um **conjunto de imagens de teste** e, em seguida, calcularemos o `MSE` entre as imagens colorizadas e as imagens originais. Faremos isso uilizando uma rotina parecida com a de treinamento, mas sem a necessidade de um otimizador (consulte [Testando o Modelo](https://rafaeldbo.github.io/project-colorization/testing) para saber mais):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(\n",
    "    images_path = path.join(actual_dir, \"data\", \"test_color\"), # caminho para a pasta com as imagens coloridas usadas no teste\n",
    "    categories_file = path.join(actual_dir, \"data\", \"categories.csv\"), # caminho para o arquivo csv com as categorias das imagens\n",
    "    size = n_test_images # quantidade de imagens a serem usadas no teste\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, # tamanho dos batches que serão gerados\n",
    "    shuffle=False # note que dessa vez, não precisamos que os batches sejam formados de maneira aleatória\n",
    ")\n",
    "\n",
    "# Carregando o modelo treinado\n",
    "ecnn = ECNN_model().to(device)\n",
    "ecnn.load_state_dict(\n",
    "    load(\n",
    "        path.join(actual_dir, \"ecnn_model.pt\"), \n",
    "        weights_only=True # esse parâmetro impede um warning\n",
    "    )[\"model_state_dict\"]\n",
    ")\n",
    "\n",
    "total_loss = 0  \n",
    "\n",
    "# Criando a barra de progresso\n",
    "total_batches = len(dataloader)\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Testing Progress\", position=0, bar_format='{l_bar}{bar:20}{r_bar}')\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    gray, color, category = batch\n",
    "\n",
    "    # Movendo os dados para o dispositivo de testes\n",
    "    gray = gray.to(device)\n",
    "    color = color.to(device)\n",
    "    category = category.to(device)\n",
    "    \n",
    "    # Passando os dados pelo modelo\n",
    "    predicted = ecnn(gray, category)\n",
    "    \n",
    "    with no_grad(): # impedindo que o gradientes sejam calulados para otimizar os testes\n",
    "        criterion = nn.MSELoss() # iniciando a função de perda\n",
    "        loss = criterion(predicted, color) # calculando a perda\n",
    "        \n",
    "        total_loss += loss.item() # somando a perda total\n",
    "\n",
    "    # Atualizando a barra de progresso\n",
    "    progress_bar.update(1)\n",
    "    progress_bar.set_postfix(batch=i+1, loss=loss.item())\n",
    "progress_bar.close()\n",
    "        \n",
    "print(\"Teste Terminado!\")\n",
    "print(f\"Erro Médio no Dataset de Teste: {round(total_loss/total_batches, 2)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_deploy_model\"> </a>\n",
    "## **Aplicando o Modelo**\n",
    "\n",
    "Agora que o modelo já foi treinado e testado, podemos utilizá-lo para colorir novas imagens. Vamos criar uma função simples que recebe uma imagem em escala de cinza e a categoria da imagem e retorna a imagem colorida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(image_path: str, category: int) -> Tensor:\n",
    "    ecnn = ECNN_model()\n",
    "    ecnn.load_state_dict(\n",
    "        load(\n",
    "            path.join(actual_dir, \"ecnn_model.pt\"), \n",
    "            weights_only=True\n",
    "        )[\"model_state_dict\"]\n",
    "    )\n",
    "    img = imread(image_path) \n",
    "\n",
    "    # Converte a imagem de RGB para LAB\n",
    "    LAB_img = from_numpy(rgb2lab(img)) \n",
    "    LAB_img = LAB_img.permute(2, 0, 1) \n",
    "\n",
    "    # Separa o layer L\n",
    "    gray_layer = LAB_img[0, :, :].unsqueeze(0) \n",
    "    \n",
    "    # Criando batch falso com os dados para passar pelo modelo\n",
    "    gray = gray_layer.unsqueeze(0).float()\n",
    "    category = torch.tensor([category])\n",
    "    \n",
    "    with no_grad(): # impedindo que o gradientes sejam calulados\n",
    "        color_layers = ecnn(gray, category)\n",
    "    \n",
    "    predicted_LAB_img = cat((gray, color_layers), 1) # concatenando os layers L e AB\n",
    "    predicted_LAB_img = predicted_LAB_img.squeeze(0) # removendo o batch falso\n",
    "    predicted_LAB_img = predicted_LAB_img.permute(1, 2, 0) # reorganizando os layes de (Cor, Largura, Altura) para (Largura, Altura, Cor)    \n",
    "    predicted_RGB_img = lab2rgb(predicted_LAB_img) # convertendo a imagem de LAB para RGB\n",
    "    \n",
    "    return predicted_RGB_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a função pronta, basta testar em uma imagem $400 \\times 400$ em preto e branco da sua escolha e ver o resultado. Caso não tenha nenhuma, disponibilizamos essa imagem de exemplo: [exemplo.jpg](https://drive.google.com/uc?export=download&id=1cLzdqNwXpkAB58mIc8GjPdD_unGculeG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = imread(\"./exemplo.jpg\")\n",
    "colorized_img = colorize(\"./exemplo.jpg\", 0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(gray_image)\n",
    "plt.title(\"Imagem em Preto e Branco\")\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(colorized_img)\n",
    "plt.title(\"Imagem Colorida\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
