{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_start\"></a>\n",
    "# **Embedded Convolutional Neural Network - Tutorial Prático**\n",
    "\n",
    "Esta é a parte prática do tutorial de construção de um modelo de colorização de imagens classificadas. Nessa `Notebook`, você será guiado a construir código do modelo **ECNN** e também os códigos necessários para **treinar** o modelo, **testar** sua acurácia e **aplicar** o modelo para colorir novas imagens.\n",
    "\n",
    "Esse tutorial possuirá pouca teoria, espera-se você já tenha lido a parte teórica nessecessária para entender o que será feito aqui presente em nosso [site](https://rafaeldbo.github.io/project-colorization/context). De qualquer maneira, as sessões necessárias para entendimento de cada etapa desse `Notebook` serão referênciadas para consulta na própria etapa. \n",
    "\n",
    "## **Sumário**\n",
    "\n",
    "1. [Preparando as Imagens](#go_to_load_images)\n",
    "2. [Construindo o Modelo](#go_to_building_model)\n",
    "\n",
    "    2.1. [Criando a Estrutura do Modelo](#go_to_model_layers)\n",
    "\n",
    "    2.2. [Função \"forward\"](#go_to_forward)\n",
    "    \n",
    "    2.3. [Modelo Completo](#go_to_ECNN_model)\n",
    "3. [Construindo a Rotina de Treinamento](#go_to_train_model)\n",
    "4. [Construindo a Rotina de Testes](#go_to_test_model)\n",
    "5. [Aplicando o Modelo](#go_to_deploy_model)\n",
    "\n",
    "## **Instalando Dependências**\n",
    " \n",
    "Antes de começarmos, caso não tenha as bibliotecas necessárias, rode a célula abaixo para instala-las ou instale utilizando o arquivo [requirements.txt](https://github.com/rafaeldbo/project-colorization/blob/main/requirements.txt) presente em nosso repositório. É recomendado o uso de um ambiente virtual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só é necessário rodar esse script uma vez para instalar as dependências necessárias\n",
    "!python -m pip install torch scikit-image matplotlib pandas numpy jupyter ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso possuia **GPU** com suporte ao `cuda` e deseja utiliza-la, verifique se ela já está disponivel utilizando o código abaixo. Caso não esteja, acesse esse [site](https://pytorch.org/get-started/locally/) e escolha a versão do `pytorch` com maior compatibilidade com sua GPU e a instale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"versão do torch: {torch.__version__}\")\n",
    "print(\"cuda disponivel!\" if torch.cuda.is_available() else \"cuda indisponível :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import time\n",
    "import pandas as pd\n",
    "from os import path, listdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor,  cat, from_numpy, save, load\n",
    "from torch.nn.functional import relu\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_load_images\"></a>\n",
    "## **Preparando as Imagens**\n",
    "\n",
    "A teoria referente a essa etapa está disponível na sessão [Entradas e Saídas do Modelo](https://rafaeldbo.github.io/project-colorization/inputs_outputs) do nosso site.\n",
    "\n",
    "Um ponto importante do modelo que criaremos nesse tutorial é que, após ser treinado por um conjunto de imagens de determinadas dimensões ($largura \\times altura$), ele só poderá ser aplicado em imagens destas mesmas dimensões. Nesse tutorial, utilizaremos as imagens do dataset [Image Colorization Dataset](https://www.kaggle.com/datasets/aayush9753/image-colorization-dataset) que possuem as dimensões $400 \\times 400$, dessa forma, todas as imagens recebidas e \"geradas\" pelo modelo possuirão essas mesmas dimensões. Além disso, como se trata de um modelo que colore imagem caregorizadas, também precisaremos de um arquivos com as categorias de cada imagem. Felizmente nosso time já preparou o aquivo [categories.csv](https://alinsperedu-my.sharepoint.com/:x:/g/personal/rafaeldbo_al_insper_edu_br/ETV6ST4HWAFFhvtF-JJ5HjsB_v9Fe3QacOhVpd3ynIYiyA?e=2W7cAB) exatamente com essa informação. Coloque tanto as pastas de imagens do dataset quanto o arquivo de categorias em uma pasta `data`, para melhor organização.\n",
    "\n",
    "Para preparar as imagens para a utilização pelo nosso modelo precisaremos criar um classe de **dataset** personalisada baseada na classe `Dataset` do `pytorch`. Ela deverá localizar todas a imagens que serão utilizadas pelo modelo e possuir uma função `__getitem__` que  caregará uma imagem de cada vez, fornecendo os dados da imagem e sua categoria. Essa estrutura será imoprtante para que, no futuro, o `DataLoader` seja capaz de utilizar essa classe **dataset** para carregar as imagens dos nossos **Batches** (caso não se lembre do que estamos falando, isso será retomado mais adiante). Para facilitar, já iremos fazer com que ela retorne separadamente o layer L e os layers AB.\n",
    "\n",
    "O código desse dataset personalizado será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "        images_path: str, # Caminho da pasta onde estão as imagens \n",
    "        categories_file: str, # Caminho do arquivo csv com as categorias das imagens\n",
    "        size: int = -1, # Quantidade de imagens a serem carregadas, sendo -1 para todas\n",
    "    ):\n",
    "        # criando uma lista com os arquivos das imagens\n",
    "        self.images_path = images_path\n",
    "        images = listdir(images_path) # listando os arquivos da pasta\n",
    "        size = size if size > 0 else len(images)\n",
    "        self.images_files = images[:size]\n",
    "        \n",
    "        # criando um dicionário com as categorias das imagens\n",
    "        df = pd.read_csv(path.join(categories_file), delimiter=';') # lendo o arquivo csv\n",
    "        df['category'] = df['category'].fillna(0) # colocando a categoria 0 para as imagens sem categoria, caso existam\n",
    "        self.categories = df.set_index('image')['category'].to_dict() # criando o dicionário\n",
    "\n",
    "    # função que retorna o tamanho do dataset\n",
    "    def __len__(self):\n",
    "        return len(self.images_files)\n",
    "\n",
    "    def __getitem__(self, \n",
    "        index: int, # Índice da imagem a ser carregada\n",
    "    ) -> tuple[Tensor, Tensor, int]:\n",
    "        \n",
    "            # lendo a imagem\n",
    "            img_file = self.images_files[index]\n",
    "            img_path = path.join(self.images_path, img_file)\n",
    "            img = imread(img_path) \n",
    "\n",
    "            # converte a imagem de RGB para LAB\n",
    "            LAB_img = from_numpy(rgb2lab(img)) \n",
    "            LAB_img = LAB_img.permute(2, 0, 1) \n",
    "\n",
    "            # separa os layers\n",
    "            gray_layer = LAB_img[0, :, :].unsqueeze(0) \n",
    "            color_layers = LAB_img[1:, :, :] \n",
    "            \n",
    "            # retornando o layer L, os layers AB e a categoria da imagem\n",
    "            return gray_layer.float(), color_layers.float(), self.categories[img_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=go_to_building_model> </a>\n",
    "## **Construindo o Modelo**\n",
    "\n",
    "A teoria referente a essa etapa está disponível em diversas sessões do site, começando pela sessão [Tipo do Modelo](https://rafaeldbo.github.io/project-colorization/model_type). Nosso modelo consistirá em um `Autoencoder` formado por `redes neurais convolucionais` e camadas de `embeddings` utilizando o layout `U-Net`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=go_to_model_layers></a>\n",
    "### **Criando a Estrutura do Modelo**\n",
    "\n",
    "Na parte convolucional da U-net (consulte as sessões [Tipos de Convoluções](https://rafaeldbo.github.io/project-colorization/convolutions) e [U-Net](https://rafaeldbo.github.io/project-colorization/unet) para saber mais), teremos:\n",
    "\n",
    "- 4 níveis nas etapas de **Encoding** e **Decoding**\n",
    "\n",
    "| `Nível`   | **Encoder**      | **Decoder**                     |\n",
    "|-----------|------------------|---------------------------------|\n",
    "| `Nível 0` | 1 layer $^{(1)}$ | 2 layer &  1 layer $^{(1, 2)}$  |\n",
    "| `Nível 1` | 32 layers        | 32 layers & 32 layers $^{(2)}$  | \n",
    "| `Nível 2` | 64 layers        | 64 layers & 64 layers $^{(2)}$  |\n",
    "| `Nível 3` | 128 layers       | 128 layer & 128 layers $^{(2)}$ |\n",
    "\n",
    "**OBS¹.:** esse \"1 layer\" se refere a layer de entrada (camada L da imagem).\n",
    "\n",
    "**OBS².:** utilizamos o simbulo $\\&$ para indicar concatenação dos layers.\n",
    "\n",
    "**OBS³.:** os quantidade de layers mostrados na tabela indicam a quantidade de layers na saída de nível de convolução. \n",
    "\n",
    "- 3 camadas de transição\n",
    "    - Uma convolucional simples de entrada 128 layers e saida 256 layers\n",
    "    - duas convolucionais de dilatação que não alteram as dimensões ou os layers\n",
    "\n",
    "- 1 camada convolucional de saida de entrada 3 layers (nível 0 do Decoder) e sáida 2 layers (objetivo)\n",
    "\n",
    "- Utilizaremos uma camada de `normalização em batches` após cada camada convolucional, menos no `nível 0` do **Decoder** e na camada de saída (consulte o tópico [Normalização em Batches](https://rafaeldbo.github.io/project-colorization/batnorm_actfunc/#normalizacao-em-batches-batch-normalization) para saber mais).\n",
    "\n",
    "Além disso, para acrescentar as informações da categoria no processo utilizaremos uma camada de `embeddings` (consulte a sessão [Camada de Embeddings](https://rafaeldbo.github.io/project-colorization/embeddings/) para saber mais) que será construida utilizando um vetor de 10 carateristicas e as 8 categorias que possuimos (as `7` que foram usadas para classificar as imagens mais a categoria `0` para as imagens sem categoria). Essa camada será conectada aos layers do `Nível 0` do **Encoder** e o `nível 3`  do **Decoder** por meio da concatenação dos layers da \"imagem\" com os layers formados pelo pelo vetor de caracteristiscas da categoria (embbeding). Não se preocupe se não entendeu, isso ficará mais claro quando aplicarmos isso durante a contrução a função de `fowarding` do modelo.\n",
    "\n",
    "``` python\n",
    "# Embeddings\n",
    "emb_size = 10\n",
    "embd = nn.Embedding(8, emb_size)\n",
    "\n",
    "# Encoder\n",
    "# Nível 1 (1+emb_size -> 32)\n",
    "conv1 = nn.Conv2d(1 + emb_size, 32, kernel_size=4, stride=2, padding=1)\n",
    "conv1_bn = nn.BatchNorm2d(32) \n",
    "\n",
    "# Nível 2 (32 -> 64)\n",
    "conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "conv2_bn = nn.BatchNorm2d(64)\n",
    "\n",
    "# Nível 3 (64 -> 128)\n",
    "conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "conv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "# Transição\n",
    "# convolução de transição (128 -> 256)\n",
    "conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "conv4_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "# primeira dilatação\n",
    "dilat1 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "dilat1_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "# segunda dilatação\n",
    "dilat2 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "dilat2_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "# Decoder\n",
    "# Nível 3 (256+emb_size -> 128)\n",
    "tconv3 = nn.ConvTranspose2d(256 + emb_size, 128, kernel_size=4, stride=2, padding=1)\n",
    "tconv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "# Nível 2 (128+128 -> 64)\n",
    "tconv2 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "tconv2_bn = nn.BatchNorm2d(64)\n",
    "\n",
    "# Nível 1 (64+64 -> 32)\n",
    "tconv1 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "tconv1_bn = nn.BatchNorm2d(32)\n",
    "\n",
    "# Nível 0 (32+32 -> 2)\n",
    "tconv0 = nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "# saída (2+1 -> 2)\n",
    "tconv_out = nn.Conv2d(3, 2, kernel_size=3, stride=1, padding=1)\n",
    "```\n",
    "\n",
    "Note que como iremos concatenar as informações do embedding com as informações da imagem, a entrada das camadas convolucionais iniciais do **Encoder** e **Decoder** terão um número de layers maior que o normal (será somado a tamanho do embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=go_to_forward> </a>\n",
    "## **Função \"forward\"**\n",
    "\n",
    "A função `forward` é a função que será chamada quando passarmos uma imagem pelo modelo. Ela é responsável por passar a imagem por todas as camadas do modelo e retornar a imagem colorida (consulte [Forwarding](https://rafaeldbo.github.io/project-colorization/unet/#fowarding) para saber mais).\n",
    "\n",
    "Antes de implementa-la, precisamos entender como será feita a concatenação das informações do embedding com as informações da imagem. Faremos isso em 4 passos:\n",
    "\n",
    "1. **Embedding da Categoria**: a categoria da imagem será passada por uma camada de embeddings que transformará a categoria em um vetor de 10 características. \n",
    "2. **Reoganização do Enbedding**: reorganizaremos o vetor de características será  para poder ser aplicado a um **batch** de imagens. Isso será feito adicionando mais uma dimensão ao vetor.\n",
    "3. **Replicação do Embedding**: o vetor de características será replicado para o tamanho da imagem. Isso será feito para que o vetor de características possa ser concatenado com a imagem.\n",
    "4. **Concatenação**: por fim, concatenaremos o vetor de características formatado com a imagem.\n",
    "\n",
    "o código responsável por fazer essas 4 etapas é:\n",
    "``` python\n",
    "embd_category = embd(category) # obtendo o vetor de características\n",
    "embd_category = embd_category.view(-1, emb_size, 1, 1) # colocando a dimensão extra\n",
    "embd_category = embd_category.repeat(1, 1, img.shape[2], img.shape[3]) # replicando o vetor\n",
    "\n",
    "img_embd = cat((img, embd_category), 1) # concatenando a imagem com o vetor formatado\n",
    "```\n",
    "\n",
    "Agora que entendemos como será feita a concatenação, podemos implementar a função `forward` do modelo. Nele, nós utilizaremos o `ReLU` como função de ativação (consulte o tópico [Funções de Ativação](https://rafaeldbo.github.io/project-colorization/batnorm_actfunc/#funcoes-de-ativacao-activation-functions) para saber mais). Os níveis serão construidos da seguinte forma:\n",
    "- **Encoder e Transição**\n",
    "    - Aplicação da camada de `convolução`;\n",
    "    - Aplicação da camada de `normalização em batches`;\n",
    "    - Aplicação da função de ativação `ReLU`.\n",
    "- **Decoder**\n",
    "    - Aplicação da camada de `convolução transposta`;\n",
    "    - Aplicação da camada de `normalização em batches`;\n",
    "    - Aplicação da função de ativação `ReLU`;\n",
    "    - Concatenação com a saida do nível correspondente do **Encoder**.\n",
    "\n",
    "Obtedno a seguinte estrutura:\n",
    "``` python\n",
    "def foward(gray, category):\n",
    "    \n",
    "    # primeiro Embedding\n",
    "    # obtendo o vetor de características formatado para a concatenação com a imagem de entrada\n",
    "    embd_decoder = embd(category)\\ \n",
    "        .view(-1, emb_size, 1, 1)\\\n",
    "        .repeat(1, 1, gray.shape[2], gray.shape[3]) \n",
    "    # concatenação da imagem de entrada com o vetor de características\n",
    "    gray_embd_encoder = cat((gray, embd_decoder), 1)\n",
    "\n",
    "    # Encoder\n",
    "    gray_conv1 = relu(conv1_bn(conv1(gray_embd_encoder))) # Nível 1\n",
    "    gray_conv2 = relu(conv2_bn(conv2(gray_conv1))) # Nível 2\n",
    "    gray_conv3 = relu(conv3_bn(conv3(gray_conv2))) # Nível 3\n",
    "\n",
    "    # Transição\n",
    "    gray_conv4 = relu(conv4_bn(conv4(gray_conv3))) # convolução de transição\n",
    "    gray_dilat1 = relu(dilat1_bn(dilat1(gray_conv4))) # primeira dilatação\n",
    "    gray_dilat2 = relu(dilat2_bn(dilat2(gray_dilat1))) # segunda dilatação\n",
    "\n",
    "    # Sesegundo Embeeding\n",
    "    # obtendo o vetor de características formatado para a concatenação com de entrada no Decoder\n",
    "    embd_decoder = embd(category)\\\n",
    "        .view(-1, emb_size, 1, 1)\\\n",
    "        .repeat(1, 1, gray_dilat2.shape[2], gray_dilat2.shape[3])\n",
    "    # concatenação da imagem de entrada do decoder com o vetor de características\n",
    "    gray_embd_decoder = cat((gray_dilat2, embd_decoder), 1) \n",
    "\n",
    "    # Decoder\n",
    "    gray_tconv3 = relu(tconv3_bn(tconv3(gray_embd_decoder))) # Nível 3\n",
    "    gray_tconv3 = cat((gray_tconv3, gray_conv3), 1) # concatenação com a saída do nível 3 do Encoder\n",
    "    gray_tconv2 = relu(tconv2_bn(tconv2(gray_tconv3))) # Nível 2\n",
    "    gray_tconv2 = cat((gray_tconv2, gray_conv2), 1) # concatenação com a saída do nível 2 do Encoder\n",
    "    gray_tconv1 = relu(tconv1_bn(tconv1(gray_tconv2))) # Nível 1\n",
    "    gray_tconv1 = cat((gray_tconv1, gray_conv1), 1) # concatenação com a saída do nível 1 do Encoder\n",
    "    gray_tconv0 = relu(tconv0(gray_tconv1)) # Nível 0\n",
    "    gray_tconv0 = cat((gray_tconv0, gray), 1) # concatenação com a imagem de entrada\n",
    "\n",
    "    output = tconv_out(gray_tconv0) # gerando as camadas AB de saída\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_ECNN_model\"> </a>\n",
    "### **Modelo Completo**\n",
    "\n",
    "Agora que temos a estrutura do modelo, podemos construir a classe do modelo. Ela será construida utilizando a classe `nn.Module` do `pytorch` como base, terá a função `forward` que acabamos de construir. Além disso, ela terá uma função `__init__` que inicializará todas as camadas do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECNN_model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.emb_size = 10\n",
    "        self.embd = nn.Embedding(8, self.emb_size)\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1 + self.emb_size, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Transition\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(256)\n",
    "        self.dilat1 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "        self.dilat1_bn = nn.BatchNorm2d(256)\n",
    "        self.dilat2 = nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=3, dilation=2)\n",
    "        self.dilat2_bn = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Decoder\n",
    "        self.tconv3 = nn.ConvTranspose2d(256 + self.emb_size, 128, kernel_size=4, stride=2, padding=1 )\n",
    "        self.tconv3_bn = nn.BatchNorm2d(128)\n",
    "        self.tconv2 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.tconv2_bn = nn.BatchNorm2d(64)\n",
    "        self.tconv1 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.tconv1_bn = nn.BatchNorm2d(32)\n",
    "        self.tconv0 = nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.tconv_out = nn.Conv2d(3, 2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        gray: Tensor,\n",
    "        category: int,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        # First Embeeding\n",
    "        embd_decoder = self.embd(category)\\\n",
    "            .view(-1, self.emb_size, 1, 1)\\\n",
    "            .repeat(1, 1, gray.shape[2], gray.shape[3])\n",
    "        gray_embd_encoder = cat((gray, embd_decoder), 1)\n",
    "\n",
    "        # Encoder\n",
    "        gray_conv1 = relu(self.conv1_bn(self.conv1(gray_embd_encoder)))\n",
    "        gray_conv2 = relu(self.conv2_bn(self.conv2(gray_conv1)))\n",
    "        gray_conv3 = relu(self.conv3_bn(self.conv3(gray_conv2)))\n",
    "\n",
    "        # Transition\n",
    "        gray_conv4 = relu(self.conv4_bn(self.conv4(gray_conv3)))\n",
    "        gray_dilat1 = relu(self.dilat1_bn(self.dilat1(gray_conv4)))\n",
    "        gray_dilat2 = relu(self.dilat2_bn(self.dilat2(gray_dilat1)))\n",
    "\n",
    "        # Second Embeeding\n",
    "        embd_decoder = self.embd(category)\\\n",
    "            .view(-1, self.emb_size, 1, 1)\\\n",
    "            .repeat(1, 1, gray_dilat2.shape[2], gray_dilat2.shape[3])\n",
    "        gray_embd_decoder = cat((gray_dilat2, embd_decoder), 1)\n",
    "\n",
    "        # Decoder\n",
    "        gray_tconv3 = relu(self.tconv3_bn(self.tconv3(gray_embd_decoder)))\n",
    "        gray_tconv3 = cat((gray_tconv3, gray_conv3), 1)\n",
    "        gray_tconv2 = relu(self.tconv2_bn(self.tconv2(gray_tconv3)))\n",
    "        gray_tconv2 = cat((gray_tconv2, gray_conv2), 1)\n",
    "        gray_tconv1 = relu(self.tconv1_bn(self.tconv1(gray_tconv2)))\n",
    "        gray_tconv1 = cat((gray_tconv1, gray_conv1), 1)\n",
    "        gray_tconv0 = relu(self.tconv0(gray_tconv1))\n",
    "        gray_tconv0 = cat((gray_tconv0, gray), 1)\n",
    "\n",
    "        output = self.tconv_out(gray_tconv0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"go_to_train_model\"> </a>\n",
    "## **Construindo a Rotina de Treinamento**\n",
    "\n",
    "A teoria referente a essa etapa está disponível em diversas sessões do site, começando pela sessão [Otimização dos Parâmetros](https://rafaeldbo.github.io/project-colorization/otim_params). Para treinar um modelo é preciso definir uma função de perda, para mensurar os erros do nosso modelo e um otimizador para corrigir os parâmtros do modelo com base nos erros encontrados (consulte os tópicos [Funções de Perda]() e [Otimizador]() para saber mais). A função de perda que usaremos será a `MSE` (Mean Squared Error) e o otimizador será o `Adam`.\n",
    "\n",
    "Para treinar o modelo, utilizaremos um `DataLoader` que carregará os dados do nosso dataset e os dividirá em **batches**. Cada **batch** será passado pelo modelo e, e em seguida, passará função de perda para mensusar os erros. O otimizador então corrigirá os parâmetros do modelo com base na função de perda. Esse processo será repetido por um número de **epochs** definido por nós (consulte [Batches e Epochs](https://rafaeldbo.github.io/project-colorization/batches_epochs) para saber mais).\n",
    "\n",
    "Por fim, salvaremos o modelo treinado para que possamos utiliza-lo posteriormente.\n",
    "\n",
    "**OBS¹.:** Nesse tutorial, não utilizaremos **multiprocessamento**, o que agilizaria o processo de treino, pois isso demandaria uma estrutura de código mais robusta, o que não é a proposta desse tutorial. Caso queira saber mais sobre **multiprocessamento**, consulte a sessão [Multiprocessamento](https://rafaeldbo.github.io/project-colorization/multiprocessing) do nosso site. O código principal que elaboramos, disponível em nosso [repositório](https://github.com/rafaeldbo/project-colorization/tree/main/code), já possui essa funcionalidade, caso queira consultar. \n",
    "\n",
    "**OBS².:** Por padrão, tentaremos treinar o modelo utilizando a `cuda` da **GPU** (caso esteja disponível). Caso não queria utilizar a `cuda`, basta fixar a variavel **device** em `cpu`.\n",
    "\n",
    "Antes de construir a rotina de treinamento, vamos definir alguns parametros que serão utilizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 64 # quantidade de imagens a serem carregadas\n",
    "batch_size = 32 # quantidade de imagens por batch\n",
    "epochs = 50 # quantidade de Epochs de treinamento\n",
    "learning_rate = 0.001 # taxa de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_dir = path.dirname(path.abspath(__name__))\n",
    "\n",
    "dataset = ImageDataset(\n",
    "     images_path = path.join(actual_dir, \"data\", \"train_color\"), # caminho para a pasta com as imagens coloridas\n",
    "     categories_file = path.join(actual_dir, \"data\", \"categories.csv\"), # caminho para o arquivo csv com as categorias das imagens\n",
    "     size = n_images # quantidade de imagens a serem carregadas\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size, # tamanho dos batches que serão gerados\n",
    "    shuffle=True # os batches serão formados de forma aleatória\n",
    ")\n",
    "\n",
    "# Escolhendo o dispositivo de treinamento\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Treinando usando a [{device_name}].\")\n",
    "device = torch.device(device_name)\n",
    "\n",
    "ecnn = ECNN_model().to(device) # iniciando o modelo\n",
    "criterion = nn.MSELoss() # iniciando a função de perda\n",
    "optimizer = Adam(ecnn.parameters(), lr=learning_rate) # iniciando o otimizador para os parêmtros do modelo e com o learning rate desejado\n",
    "\n",
    "running_losses = [] # lista para armazenar as perdas de cada epoch\n",
    "\n",
    "print(f\"Número de Parâmetros Treinaveis: {sum(p.numel() for p in ecnn.parameters())}\")\n",
    "\n",
    "# criando a barra de progresso\n",
    "total_batches = epochs * len(dataloader)\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Progresso do Treinamento\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs): # para cada epoch...\n",
    "    epoch_running_loss = 0\n",
    "    for i, data in enumerate(dataloader): # para cada batch...\n",
    "        gray, color, category = data # extraindo os dados da batch\n",
    "\n",
    "        # movendo os dados para o dispositivo de treinamento\n",
    "        gray = gray.to(device) \n",
    "        color = color.to(device)\n",
    "        category = category.to(device)\n",
    "    \n",
    "        optimizer.zero_grad() # zerando os gradientes do otimizador\n",
    "        outputs = ecnn(gray, category) # passando os dados pelo modelo\n",
    "\n",
    "        loss = criterion(outputs, color) # calculando a perda\n",
    "        loss.backward() # derivando a perda\n",
    "        optimizer.step() # atualizando os parâmetros do modelo com base na perda\n",
    "\n",
    "        epoch_running_loss += loss.item()\n",
    "        \n",
    "        # Atualizando a barra de progresso\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(epoch=epoch+1, batch=i+1, loss=loss.item())\n",
    "        \n",
    "    running_losses.append(epoch_running_loss)\n",
    "    \n",
    "progress_bar.close()\n",
    "print(f\"Treinamento terminado\")\n",
    "# Salvando o modelo\n",
    "save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": ecnn.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"time\": time.time() - start,\n",
    "        \"running_losses\": running_losses\n",
    "    }, \n",
    "    \"ecnn_model.pt\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
