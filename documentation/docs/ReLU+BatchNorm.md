Em cada camada, seja ela de convolução ou convolução transposta, teremos vários nós. Como é característico da rede neural, um nó receberá como entrada os valores de saída de nós da camada anterior. O valor de saída do nó atual dependerá de uma soma de um valor "bias" com os valores ponderados da camada anterior. 

Entretanto, esse valor será ajustado por uma função de ativação, que determina se o valor calculado irá ou não ser propagado para a próxima camada. No nosso modelo, a função de ativação é a ReLU, que determina que a saída do nó será a soma calculada caso ela seja maior do que 0, e 0 caso contrário. Essa função de ativação reduz o problema do desaparecimento de gradientes, que causaria perda de informação na rede.

Uma vez que o aprendizado do modelo é justamente determinar os pesos ideias para essa soma, o valor de saída do nó mudará a cada interação.